{"./":{"url":"./","title":"Introduction","keywords":"","body":"Ops Ops 是一个运维工具项目。它的目标是提供一个简单的运维工具，让运维人员可以快速地完成运维工作。 生产实践 日构建 2k+ 的 CICD 集群 海外集群 40+ 个集群 AI 算力集群 20+ 个集群 支持 ARM、X86 架构 设计 对象定义 Host，主机。可以是云主机、裸金属机器，通过 SSH 能够访问到的机器。 Cluster，Kubernetes 集群。通过 kubectl 能够访问的 Kubernetes 集群。 Task，组合多个 File 和 Shell 的任务。 Pipeline，组合多个 Task 的任务。 核心操作 File，文件的上传和下发。 Shell，执行脚本。 组件 ops-cli：是可以单独使用的命令行工具，辅助运维人员在命令行终端完成一些自动化的运维工作 ops-server：一个 HTTP 服务，用于提供 HTTP API，提供有一个 Dashboard 的界面 ops-controller：以 Operator 的形式管理主机、集群、任务、流水线等资源 多集群支持 在实践中，建议： 将当前集群的主机创建为 Host 可以创建多个 Cluster，拥有的 Cluster 对象即为纳管的集群 Task、Pipeline 对象会自动同步到集群下的全部 Cluster 集群中，无需人工触发。 当下发一个流水线任务时，需要创建一个 PipelineRun 对象。PipelineRun 是可以跨集群的，而 TaskRun 不行。 Controller 会根据 PipelineRun 中设置的 cluster 字段，将 PipelineRun 分发到指定的集群中，由集群内的 Controller 执行具体的任务，再将 PipelineRun 的状态更新到主集群内的 PipelineRun 对象中。 事件驱动 建议在每个集群中安装一个 Nats 组件，通过边缘集群的模式，可以将全部的事件汇总到一个集群，或者若干个网络分区的集群。 在事件中，主要定义了以下 Topic: 探活类：每个主机、集群会有定时检测，能够看到探活的事件 执行任务类：执行 TaskRun、PipelineRun 任务的事件 巡检类：TaskRun 执行定时任务巡检任务时，会推送相关的检测事件 Webhook 类：用户自定义的一些运维事件，告警、通知等 "},"opscli.html":{"url":"opscli.html","title":"Opscli","keywords":"","body":"opscli 功能简介 批量远程执行命令 批量分发文件 创建 Ops Controller CRD 资源 主要分为三类 CRD 资源: Host, Cluster, Task 支持的操作系统 Linux macOS "},"opscli-install.html":{"url":"opscli-install.html","title":"Install","keywords":"","body":"opscli 安装 安装 单机安装 国内使用: PROXY=https://ghfast.top/ curl -sfL $PROXY/https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh | VERSION=latest PROXY=$PROXY sh - 国外使用: curl -sfL https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh | VERSION=latest sh - 批量安装 将需要安装的全部主机 ip 都写入到 hosts.txt 文件中，然后使用 opscli shell 命令批量安装，凭证默认为当前用户的 ~/.ssh/id_rsa。 国内使用: /usr/local/bin/opscli shell --content \"curl -sfL https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh | VERSION=latest sh -\" -i hosts.txt 国外使用: /usr/local/bin/opscli shell --content \"curl -sfL https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh | VERSION=latest sh -\" -i hosts.txt 版本升级 单机 sudo /usr/local/bin/opscli upgrade 批量 /usr/local/bin/opscli shell --content \"sudo /usr/local/bin/opscli upgrade\" -i hosts.txt 自动补全 bash echo 'source >~/.bashrc zsh echo 'source >~/.zshrc 配置 opscli 支持通过 config 命令进行配置管理，允许您设置和管理在所有 CLI 命令中使用的配置值。 配置文件位置 配置文件存储在 ~/.ops/opscli/config（YAML 格式）。 支持的配置项 proxy: 网络请求的代理 URL（例如：https://ghfast.top/） runtimeimage: Kubernetes 任务的默认运行时镜像（例如：ubuntu:22.04） 配置命令 设置配置: opscli config set opscli config set proxy https://ghfast.top/ opscli config set runtimeimage ubuntu:22.04 获取配置: opscli config get opscli config get proxy 列出所有配置: opscli config list opscli config list # 输出： # proxy = https://ghfast.top/ # runtimeimage = (not set) 删除配置: opscli config unset opscli config unset proxy 配置优先级 配置值遵循以下优先级顺序（从高到低）： CLI 参数（最高优先级） 命令行标志，如 --proxy 或 --runtimeimage 示例：opscli task --filepath task.yaml --proxy https://cli-proxy.com 环境变量 PROXY: 代理 URL DEFAULT_RUNTIME_IMAGE: 默认运行时镜像 示例：export PROXY=https://env-proxy.com 配置文件（~/.ops/opscli/config） 通过 opscli config set 设置的值 示例：opscli config set proxy https://config-proxy.com 默认值（最低优先级） 内置默认值 Proxy: https://ghproxy.chenshaowen.com/ Runtime Image: ubuntu:22.04 使用示例 # 示例 1: 使用配置文件 opscli config set proxy https://ghfast.top/ opscli upgrade --manifests # 自动使用配置文件中的 proxy # 示例 2: 使用环境变量覆盖 export PROXY=https://env-proxy.com opscli upgrade --manifests # 使用环境变量 # 示例 3: 使用 CLI 参数覆盖（最高优先级） opscli upgrade --proxy https://cli-proxy.com # 使用 CLI 参数 更多 /usr/local/bin/opscli --help "},"opscli-shell.html":{"url":"opscli-shell.html","title":"Shell","keywords":"","body":"opscli shell command 指定操作目标清单 指定主机 -i 1.1.1.1 通过 --username 指定用户名，--password 指定密码。 批量主机 通过文件指定: -i hosts.txt cat hosts.txt 1.1.1.1 2.2.2.2 opscli 会从每行中正则匹配 ip 地址，作为目标地址。 通过逗号分割指定: -i 1.1.1.1,2.2.2.2 集群全部节点 -i ~/.kube/config --nodename all -i 默认值为 ~/.kube/config。 集群指定节点 -i ~/.kube/config --nodename node1 node1 为节点名称。 查看集群镜像 单机 /usr/local/bin/opscli task -f ~/.ops/tasks/list-podimage.yaml --namespace all 集群批量操作 全部节点 opscli shell --content \"uname -a\" --nodename all 指定节点 opscli shell --content \"uname -a\" --nodename node1 指定 kubeconfig 默认 kubeconfig 为 ~/.kube/config，可以通过 -i 参数指定。 opscli shell -i ~/Documents/opscli/prod --content \"uname -a\" --nodename node1 "},"opscli-task.html":{"url":"opscli-task.html","title":"Task","keywords":"","body":"opscli task command -i 指定操作目标清单 指定主机 -i 1.1.1.1 通过 --username 指定用户名，--password 指定密码。 批量主机 -i hosts.txt cat hosts.txt 1.1.1.1 2.2.2.2 opscli 会从每行中正则匹配 ip 地址，作为目标地址。 集群全部节点 -i ~/.kube/config --nodename all -i 默认值为 ~/.kube/config。 集群指定节点 -i ~/.kube/config --nodename node1 node1 为节点名称。 更新 /etc/hosts 主机 远程到主机 1.1.1.1 ，更新 /etc/hosts 文件。 /usr/local/bin/opscli task -f ~/.ops/tasks/set-hosts.yaml --ip 1.2.3.4 --domain test.com --i 1.1.1.1 --port 2222 --username root 如果需要清理加上 --clear 参数即可。 集群全部节点 /usr/local/bin/opscli task -f ~/.ops/tasks/set-hosts.yaml --ip 1.2.3.4 --domain test.com --i ~/.kube/config --nodename all 集群指定节点 /usr/local/bin/opscli task -f ~/.ops/tasks/set-hosts.yaml --ip 1.2.3.4 --domain test.com --i ~/.kube/config --nodename node1 应用安装 安装 Istio /usr/local/bin/opscli task -f ~/.ops/tasks/app-istio.yaml --version 1.13.7 --kubeconfig /etc/kubernetes/admin.conf --version 默认值为 1.13.7，--kubeconfig 默认值为 /etc/kubernetes/admin.conf。 卸载 Istio /usr/local/bin/opscli task -f ~/.ops/tasks/app-istio.yaml --version 1.13.7 --kubeconfig /etc/kubernetes/admin.conf --action delete 上传文件 上传到 Server /usr/local/bin/opscli task -f tasks/file-upload.yaml --fileapi https://gh-uploadapi.chenshaowen.com/api/v1/files --localfile dockerfile > Run Task ops-system/file-upload on 127.0.0.1 (1/1) upload file Please use the following command to download the file: opscli file --fileapi https://gh-uploadapi.chenshaowen.com/api/v1/files --aeskey a9f891afe71fda777b05a7063068360a914e83848d7da46d7513aee86c053f6c --direction download --remotefile https://gh-uploadapi.chenshaowen.com/uploadbases/cdn0/raw/1721615659-dockerfile.aes 上传到 S3 /usr/local/bin/opscli task -f tasks/file-upload.yaml --ak xxx --sk xxx --region beijing --endpoint ks3-cn-beijing.ksyun.com --bucket xxx --localfile dockerfile --remotefile s3://dockerfile 下载文件 从 Server 下载 /usr/local/bin/opscli task -f task -f tasks/file-download.yaml --fileapi https://gh-uploadapi.chenshaowen.com/api/v1/files --aeskey a9f891afe71fda777b05a7063068360a914e83848d7da46d7513aee86c053f6c --remotefile https://gh-uploadapi.chenshaowen.com/uploadbases/cdn0/raw/1721615659-dockerfile.aes --localfile dockerfile1 > Run Task ops-system/file-download on 127.0.0.1 (1/1) download file success download https://gh-uploadapi.chenshaowen.com/uploadbases/cdn0/raw/1721615659-dockerfile.aes to dockerfile1 从 S3 下载 /usr/local/bin/opscli task -f tasks/file-download.yaml --ak xxx --sk xxx --region beijing --endpoint ks3-cn-beijing.ksyun.com --bucket xxx --localfile dockerfile2 --remotefile s3://dockerfile > Run Task ops-system/file-download on 127.0.0.1 (1/1) download file success download s3 dockerfile to dockerfile2 "},"opscli-file.html":{"url":"opscli-file.html","title":"File","keywords":"","body":"opscli file command 主机 - 本地与对象存储互传文件 设置 AK\\SK export ak= export sk= 上传本地文件 ./tmp.log 到对象存储 s3://logs/tmp.log /usr/local/bin/opscli file --direction upload --localfile ./tmp.log --remotefile s3://logs/tmp.log --bucket obs-test --bucket 为 S3 bucket 名称，--region 为 S3 bucket 所在区域，--endpoint 为 S3 bucket 的 endpoint，--direction 为上传方向，--localfile 为本地文件，--remotefile 为远程文件。 下载 S3 s3://logs/tmp.log 到本地文件 ./tmp1.log /usr/local/bin/opscli file --direction download --localfile ./tmp1.log --remotefile s3://logs/tmp.log --bucket obs-test 清理 AK\\SK unset ak unset sk 主机 - 本地与 API Server 互传文件 提供本地加解密，与服务器端进行文件传输 上传 /usr/local/bin/opscli file --direction upload --fileapi https://gh-uploadapi.chenshaowen.com/api/v1/files --localfile ./tmp.log Please use the following command to download the file: opscli file --fileapi https://gh-uploadapi.chenshaowen.com/api/v1/files --direction download --remotefile https://download_url_link.com.aes 这里的 fileapi 提供上传服务，aeskey 为空字符串时自动生成一个随机秘钥，如果不设置 aeskey 默认为 unset 将不会进行文件加密。 下载 /usr/local/bin/opscli file --fileapi https://gh-uploadapi.chenshaowen.com/api/v1/files --aeskey xxx --direction download --remotefile https://download_url_link.com.aes 集群 - 本地与 API Server 互传文件 上传 /usr/local/bin/opscli file -i ~/.kube/config --nodename node1 --direction upload --fileapi https://gh-uploadapi.chenshaowen.com/api/v1/files --aeskey \"\" --localfile /root/tmp.log --runtimeimage shaowenchen/ops-cli 下载 /usr/local/bin/opscli file -i ~/.kube/config --nodename xxx --direction download --fileapi https://gh-uploadapi.chenshaowen.com/api/v1/files --aeskey xxx --localfile /root/tmp1.log --remotefile https://gh-uploadapi.chenshaowen.com/uploadbases/cdn0/raw/1721621949-tmp.log.aes --runtimeimage shaowenchen/ops-cli 集群 - 本地与对象存储互传文件 上传 /usr/local/bin/opscli file -i ~/.kube/config --nodename xxx --direction upload --ak xxx --sk xxx --region beijing --endpoint ks3-cn-beijing.ksyun.com --bucket multimodal --localfile /root/tmp.log --remotefile s3://logs/tmp.log --runtimeimage shaowenchen/ops-cli 下载 /usr/local/bin/opscli file -i ~/.kube/config --nodename xxx --direction download --ak xxx --sk xxx --region beijing --endpoint ks3-cn-beijing.ksyun.com --bucket multimodal --localfile /root/tmp2.log --remotefile s3://logs/tmp.log --runtimeimage shaowenchen/ops-cli 集群 - 镜像文件拷贝到本地 /usr/local/bin/opscli file -i ~/.kube/config --nodename xxx --direction download --localfile /root/opscli-copy --remotefile shaowenchen/ops-cli:latest:///usr/local/bin/opscli "},"opscli-case.html":{"url":"opscli-case.html","title":"Case","keywords":"","body":"opscli 使用案例 在 kubectl pod 中测试指定节点的磁盘 IO 性能 安装 opscli for alpine sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories apk add curl curl -sfL https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh |VERSION=latest sh - 在节点安装 fio opscli shell --content \"apt-get install fio -y\" --nodename node1 在节点上测试磁盘 IO 性能 opscli task -f ~/.ops/tasks/get-diskio-byfio.yaml --size 1g --filename=/tmp/testfile --nodename node1 其中 size 为测试文件大小，filename 为测试文件路径，nodename 为测试节点名称。 (1/8) Rand_Read_Testing read: IOPS=105k, BW=410MiB/s (430MB/s)(1024MiB/2498msec) -> 4k 随机读 410 MiB/s (2/8) Rand_Write_Testing write: IOPS=55.9k, BW=218MiB/s (229MB/s)(1024MiB/4688msec) -> 4k 随机写 218 MiB/s (3/8) Sequ_Read_Testing read: IOPS=51.8k, BW=6481MiB/s (6796MB/s)(1024MiB/158msec) -> 128k 顺序读 6481 MiB/s (4/8) Sequ_Write_Testing write: IOPS=30.7k, BW=3835MiB/s (4022MB/s)(1024MiB/267msec) -> 128k 顺序写 3835 MiB/s (5/8) Rand_Read_IOPS_Testing read: IOPS=80.4k, BW=314MiB/s (329MB/s)(1024MiB/3261msec) -> 4k 下读 IOPS 为 80.4k (6/8) Rand_Write_IOPS_Testing write: IOPS=83.4k, BW=326MiB/s (342MB/s)(1024MiB/3143msec) -> 4k 下写 IOPS 为 83.4k (7/8) Rand_Read_Latency_Testing lat (usec): min=34, max=457722, avg=57.78, stdev=1630.32 -> 4k 读延时为 57.78 us (8/8) Rand_Write_Latency_Testing lat (usec): min=35, max=664838, avg=385.12, stdev=5335.64 -> 4k 写延时为 385.12 us 给集群 GPU 主机配置巡检 在全部 master 节点上安装 Opscli opscli task -f ~/.ops/tasks/install-opscli.yaml -i master-ips.txt 在能 ssh 全部节点的机器上，创建访问主机的 ssh 密钥 kubectl -n ops-system create secret generic host-secret --from-file=privatekey=/root/.ssh/id_rsa 添加全部 task 模板 kubectl apply -f ~/.ops/tasks/ 自动发现主机 kubectl apply -f - 自动打上标签 kubectl apply -f - GPU 掉卡巡检 kubectl apply -f - GPU ECC 巡检 kubectl apply -f - GPU Fabric 巡检 kubectl apply -f - GPU Zombie 巡检 kubectl apply -f - 定时清理磁盘 kubectl apply -f - "},"opsserver.html":{"url":"opsserver.html","title":"Opsserver","keywords":"","body":"ops-server 功能简介 ops-server 是一个 HTTP 服务，提供了一些 RESTful API，用于对外提供 API 服务。 这样的用途包括： 通过 HTTP API 来批量远程执行命令 通过 HTTP API 来批量分发文件 通过 HTTP API 来创建 Ops Controller CRD 资源 关于权限 默认密码是 ops 可以通过给 Server 服务设置环境变量 SERVER_TOKEN 自定义密码。 对象管理 任务执行 "},"opscontroller.html":{"url":"opscontroller.html","title":"Opscontroller","keywords":"","body":"Ops-controller-manager 功能简介 ops-controller-manager 是一个 Kubernetes Operator。它提供了三种对象：Host, Cluster, Task。 Host Host 对象用于描述一个主机，比如主机名，IP 地址，SSH 用户名，SSH 密码，SSH 私钥等。 Cluster Cluster 对象用于描述一个集群，比如集群名，集群的主机数量、Pod 数量、负载、CPU、内存等。 Task Task 对象用于描述一个任务，比如一次性任务、周期任务。 安装 前置要求 Kubernetes 1.19+ Helm 3.0+ NATS 服务器（可选，但推荐用于事件流） 安装 Helm curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash 添加 Helm 仓库 helm repo add ops https://www.chenshaowen.com/ops/charts helm repo update 安装 ops-controller-manager 基础安装： 使用默认配置安装： helm install myops ops/ops --version 1.2.0 --namespace ops-system --create-namespace 使用自定义值安装： 可以使用 --set 参数自定义配置： helm install myops ops/ops --version 1.2.0 \\ --namespace ops-system \\ --create-namespace \\ --set controller.image.repository=\"shaowenchen/ops-controller-manager\" \\ --set controller.image.pullPolicy=\"Always\" \\ --set controller.image.tag=\"latest\" \\ --set controller.env.activeNamespace=\"ops-system\" \\ --set controller.env.defaultRuntimeImage=\"ubuntu:22.04\" \\ --set controller.replicaCount=2 \\ --set server.image.repository=\"shaowenchen/ops-server\" \\ --set server.image.pullPolicy=\"Always\" \\ --set server.image.tag=\"latest\" \\ --set server.replicaCount=2 \\ --set server.autoscaling.minReplicas=2 \\ --set server.autoscaling.maxReplicas=4 \\ --set event.cluster=\"mycluster\" \\ --set event.endpoint=\"http://app:password@nats-headless.ops-system.svc:4222\" 使用 values 文件安装： 创建自定义 values 文件以进行更复杂的配置： # my-values.yaml event: cluster: \"mycluster\" endpoint: \"http://app:password@nats-headless.ops-system.svc:4222\" controller: replicaCount: 2 image: repository: shaowenchen/ops-controller-manager pullPolicy: Always tag: \"latest\" env: activeNamespace: \"ops-system\" defaultRuntimeImage: \"ubuntu:22.04\" server: replicaCount: 2 image: repository: shaowenchen/ops-server pullPolicy: Always tag: \"latest\" autoscaling: enabled: true minReplicas: 2 maxReplicas: 4 targetCPUUtilizationPercentage: 80 prometheus: enabled: true 然后使用 values 文件安装： helm install myops ops/ops --version 1.2.0 \\ --namespace ops-system \\ --create-namespace \\ -f my-values.yaml 查看安装结果 安装后，检查 Pod 状态以确保一切正常运行： # 检查 Pod kubectl get pods -n ops-system # 检查 Service kubectl get svc -n ops-system # 检查 Deployment kubectl get deployments -n ops-system 升级安装 升级现有安装： helm upgrade myops ops/ops --version 1.2.0 --namespace ops-system 或使用自定义值： helm upgrade myops ops/ops --version 1.2.0 \\ --namespace ops-system \\ -f my-values.yaml 卸载 卸载 ops-controller-manager： helm uninstall myops --namespace ops-system 配置 命名空间配置 默认情况下，ops-controller-manager 只会处理 ops-system 命名空间下的 CRD 资源。 如果需要变更，可以修改 Helm values 中的 controller.env.activeNamespace 值。如果为空，则会处理所有命名空间的资源。 使用 --set： helm install myops ops/ops --version 1.2.0 \\ --namespace ops-system \\ --create-namespace \\ --set controller.env.activeNamespace=\"\" 使用 values 文件： controller: env: activeNamespace: \"\" # 空值表示所有命名空间 事件配置 为 controller 和 server 配置 NATS 事件端点： controller: env: eventCluster: \"default\" eventEndpoint: \"http://app:password@nats-headless.ops-system.svc:4222\" server: env: eventCluster: \"default\" eventEndpoint: \"http://app:password@nats-headless.ops-system.svc:4222\" 资源配置 配置资源限制和请求： resources: limits: cpu: 2000m memory: 4096Mi requests: cpu: 1000m memory: 2048Mi 也可以为 server 单独配置资源： server: resources: limits: cpu: 1000m memory: 2048Mi requests: cpu: 500m memory: 1024Mi 监控配置 启用 Prometheus 监控（创建 ServiceMonitor 资源）： prometheus: enabled: true 自动扩缩容配置 启用水平 Pod 自动扩缩容： autoscaling: enabled: true minReplicas: 2 maxReplicas: 10 targetCPUUtilizationPercentage: 80 targetMemoryUtilizationPercentage: 80 故障排查 查看 Pod 日志 # Controller 日志 kubectl logs -n ops-system deployment/myops-ops # Server 日志 kubectl logs -n ops-system deployment/myops-ops-server 检查指标端点 # Controller 指标 kubectl port-forward -n ops-system svc/myops-ops-controller-metrics 8080:8080 curl http://localhost:8080/metrics # Server 指标 kubectl port-forward -n ops-system svc/myops-ops-server 8080:80 curl http://localhost:8080/metrics 检查 ServiceMonitor kubectl get servicemonitor -n ops-system 更多详细配置选项，请参阅 Helm Chart README。 "},"opscontroller-host.html":{"url":"opscontroller-host.html","title":"Host","keywords":"","body":"Ops-controller-manager host 对象 直接使用 create 子命令创建 /usr/local/bin/opscli create host --name dev1 -i 1.1.1.1 --port 2222 --username root --password xxx --namespace ops-system 使用 yaml 文件创建 apiVersion: crd.chenshaowen.com/v1 kind: Host metadata: name: dev1 namespace: ops-system spec: address: 1.1.1.1 port: 2222 privatekey: base64 encoded private key username: root privatekeypath: ~/.ssh/id_rsa timeoutseconds: 10 查看对象 kubectl get hosts dev1 -n ops-system NAME HOSTNAME ADDRESS DISTRIBUTION ARCH CPU MEM DISK HEARTTIME HEARTSTATUS dev1 node1 1.1.1.1 centos x86_64 4 7.8G 52G 54s successed "},"opscontroller-cluster.html":{"url":"opscontroller-cluster.html","title":"Cluster","keywords":"","body":"Ops-controller-manager cluster 对象 直接使用 create 子命令创建 /usr/local/bin/opscli create cluster -i ~/.kube/config --name dev1 --namespace ops-system 使用 yaml 文件创建 apiVersion: crd.chenshaowen.com/v1 kind: Cluster metadata: name: dev1 namespace: ops-system spec: config: base64 encoded kubeadm config server: https://1.1.1.1:6443 查看对象 kubectl get cluster dev1 -n ops-system NAME SERVER VERSION NODE RUNNING TOTALPOD CERTDAYS STATUS dev1 https://1.1.1.1:6443 v1.21.0 1 15 16 114 successed "},"opscontroller-task.html":{"url":"opscontroller-task.html","title":"Task","keywords":"","body":"Ops-controller-manager task 对象 直接使用 create 子命令创建 /usr/local/bin/opscli create task --name t1 --typeref host --nameref dev1 --filepath ./task/get-osstaus.yaml 通过 --typeref host --nameref dev1 指定任务执行的主机。 使用 yaml 文件创建 如果不指定 typeref ，那么任务将在 ops-controller-manager pod 中执行。 下面是一个定时任务，每分钟检查一次 http 状态码，如果不是 200，那么发送通知。 apiVersion: crd.chenshaowen.com/v1 kind: Task metadata: name: alert-http-status-dockermirror namespace: ops-system spec: desc: alert crontab: \"*/1 * * * *\" variables: url: default: http://1.1.1.1:5000/ expect: default: \"200\" message: default: ${url} http status is not ${expect} steps: - name: get-status content: curl -I -m 10 -o /dev/null -s -w %{http_code} ${url} - name: notification when: ${steps.get-status.output} != ${expect} content: | curl -X POST 'https://365.kdocs.cn/woa/api/v1/webhook/send?key=xxx' -H 'content-type: application/json' -d '{ \"msgtype\": \"text\", \"text\": { \"content\": \"${message}\" } }' 查看对象 kubectl get task t1 -n ops-system kubectl get task -n ops-system NAME CRONTAB TYPEREF NAMEREF NODENAME ALL STARTTIME RUNSTATUS alert-http-status-dockermirror */1 * * * * 导出结果变量 默认输出变量： 每个 step 的输出会自动在后续 step 中可用。可以直接引用或使用路径语法： ${output} 或 ${result} - 引用前一个 step 的输出（直接引用，推荐） ${steps.{stepName}.output} - 通过名称引用指定 step 的输出（路径引用） 示例（直接引用）： steps: - name: get-status content: curl -I -m 10 -o /dev/null -s -w %{http_code} ${url} - name: notification when: ${output} != ${expect} # 直接引用（推荐） content: echo \"状态码是 ${output}\" 或使用路径语法： steps: - name: get-status content: curl -I -m 10 -o /dev/null -s -w %{http_code} ${url} - name: notification when: ${steps.get-status.output} != ${expect} # 路径引用 content: echo \"状态码是 ${steps.get-status.output}\" 使用 OPS_RESULT 标记导出： 如果需要将结果导出供 Pipeline 中的其他任务使用，可在 step 输出中使用 OPS_RESULT: 标记： steps: - name: build-step content: | docker build -t myapp:v1.0.0 . echo \"OPS_RESULT:image=registry.example.com/myapp:v1.0.0\" echo \"OPS_RESULT:tag=v1.0.0\" 支持的格式： OPS_RESULT:key=value OPS_RESULT:key:value OPS_RESULT:{\"key\":\"value\"} (JSON 格式，支持多个结果) 使用 key:value 格式导出（向后兼容）： 最后一个 step 的输出如果是 key:value 格式，会自动导出： steps: - name: build-step content: | docker build -t myapp:v1.0.0 . echo \"image:registry.example.com/myapp:v1.0.0\" "},"opscontroller-pipeline.html":{"url":"opscontroller-pipeline.html","title":"Pipeline","keywords":"","body":"Ops-controller-manager Pipeline 对象 Pipeline 对象定义了一系列按顺序执行的任务。Pipeline 中的任务可以通过路径引用使用前面任务的结果。 使用 yaml 文件创建 apiVersion: crd.chenshaowen.com/v1 kind: Pipeline metadata: name: app-deploy namespace: ops-system spec: desc: \"部署应用到生产环境\" variables: namespace: value: \"production\" tasks: - name: prepare-config taskRef: prepare-config-task results: config: generate-step version: generate-step - name: deploy-app taskRef: deploy-app-task variables: config: ${config} version: ${version} - name: verify-deploy taskRef: verify-deploy-task variables: version: ${version} 说明： desc: Pipeline 描述 variables: Pipeline 级别的变量 tasks: 按顺序执行的任务列表： name: 任务在 Pipeline 中的名称（用于路径引用） taskRef: 引用的 Task 对象 results: 结果键到 step 名称的映射，定义要导出的 step 输出 variables: 任务特定变量（由 controller 自动填充）。如果任务变量有默认值且 Pipeline 中有同名变量，会自动填充到这里。前面任务的结果也会自动作为变量可用 自动变量填充： 当 Pipeline 被创建或更新时，controller 会自动为每个任务填充 variables： 如果任务变量有默认值且 Pipeline 中有同名变量，会将 Pipeline 的值填充到 TaskRef.variables 中 这样所有变量值都会在 Pipeline spec 中明文展示 变量引用 前面任务的结果会自动添加到 Pipeline 变量中，可以直接引用： variables: config: ${config} # 直接引用（推荐） version: ${version} # 直接引用（推荐） 也可以使用路径语法进行显式引用： variables: config: ${tasks.prepare-config.results.config} # 路径引用 version: ${tasks.prepare-config.results.version} # 路径引用 查看对象 kubectl get pipeline -n ops-system "},"opscontroller-metrics.html":{"url":"opscontroller-metrics.html","title":"Metrics","keywords":"","body":"Ops 监控指标 Ops 暴露 Prometheus 指标用于监控 controller 和 server 组件。 指标端点 Controller: :9090/metrics Server: :9090/metrics Controller 指标 资源信息指标 这些指标在每次 reconcile 时暴露资源信息。 指标 标签 描述 ops_controller_task_info namespace, name, desc, host, runtime_image Task 资源信息（仅静态字段） ops_controller_task_status namespace, name Task 资源状态（动态字段） ops_controller_pipeline_info namespace, name, desc Pipeline 资源信息（仅静态字段） ops_controller_pipeline_status namespace, name Pipeline 资源状态（动态字段） ops_controller_host_info namespace, name, address Host 资源信息（仅静态字段） ops_controller_host_status namespace, name, hostname, distribution, arch, status Host 资源状态（动态字段） ops_controller_cluster_info namespace, name, server Cluster 资源信息（仅静态字段） ops_controller_cluster_status namespace, name, version, status, node, pod_count, running_pod, cert_not_after_days Cluster 资源状态（动态字段） ops_controller_eventhooks_info namespace, name, type, subject, url EventHooks 资源信息（仅静态字段） ops_controller_eventhooks_status namespace, name, keyword, event_id EventHooks 资源状态（动态字段，包含触发信息） TaskRun 指标 指标 标签 描述 ops_controller_taskrun_info namespace, name, taskref, crontab TaskRun 资源信息（仅静态字段） ops_controller_taskrun_status namespace, name, status TaskRun 资源状态（动态字段） ops_controller_taskrun_start_time namespace, name, taskref TaskRun 开始时间（unix 时间戳） ops_controller_taskrun_end_time namespace, name, taskref TaskRun 结束时间（unix 时间戳） ops_controller_taskrun_duration_seconds namespace, name, taskref, status TaskRun 运行时长（秒） PipelineRun 指标 指标 标签 描述 ops_controller_pipelinerun_info namespace, name, pipelineref, crontab PipelineRun 资源信息（仅静态字段） ops_controller_pipelinerun_status namespace, name, status PipelineRun 资源状态（动态字段） ops_controller_pipelinerun_start_time namespace, name, pipelineref PipelineRun 开始时间（unix 时间戳） ops_controller_pipelinerun_end_time namespace, name, pipelineref PipelineRun 结束时间（unix 时间戳） ops_controller_pipelinerun_duration_seconds namespace, name, pipelineref, status PipelineRun 运行时长（秒） 运行次数指标 运行次数可以通过 _info 和 _status 指标统计： 按 taskref 和 status 统计 TaskRun 总数: count by (taskref, status) (ops_controller_taskrun_info{namespace=\"$namespace\"} == 1) * on(namespace, name) group_left(status) ops_controller_taskrun_status{namespace=\"$namespace\"} 按 pipelineref 和 status 统计 PipelineRun 总数: count by (pipelineref, status) (ops_controller_pipelinerun_info{namespace=\"$namespace\"} == 1) * on(namespace, name) group_left(status) ops_controller_pipelinerun_status{namespace=\"$namespace\"} EventHooks 指标 EventHooks 触发信息记录在 ops_controller_eventhooks_status 指标中，包含 keyword 和 event_id 标签。 Reconcile 指标 指标 标签 描述 ops_controller_reconcile_total controller, namespace, result reconcile 操作总次数 ops_controller_reconcile_errors_total controller, namespace, error_type reconcile 错误总次数 Controller 资源指标 指标 标签 描述 ops_controller_resource_goroutines pod Controller goroutine 数量 ops_controller_resource_cpu_usage_seconds_total pod Controller CPU 使用量（秒，累计值，从 cgroup 读取） ops_controller_resource_memory_usage_bytes pod Controller 内存使用量（字节，从 cgroup 读取） ops_controller_uptime_seconds pod Controller 运行时间（秒） ops_controller_info pod, version, build_date Controller 信息 Server 指标 资源指标 指标 标签 描述 ops_server_resource_goroutines pod Server goroutine 数量 ops_server_resource_cpu_usage_seconds_total pod Server CPU 使用量（秒，累计值，从 cgroup 读取） ops_server_resource_memory_usage_bytes pod Server 内存使用量（字节，从 cgroup 读取） 吞吐量指标 指标 标签 描述 ops_server_throughput_http_requests_total method, path, status_code HTTP 请求总数 ops_server_throughput_api_requests_total endpoint, namespace, status API 请求总数 ops_server_throughput_api_errors_total endpoint, namespace, error_type API 错误总数 Server 信息指标 指标 标签 描述 ops_server_info pod, version, build_date Server 信息 ops_server_uptime_seconds pod Server 运行时间（秒） 查询示例 获取所有运行中的 TaskRun ops_controller_taskrun_info{status=\"Running\"} 按状态统计 Task 运行次数 count by (taskref, status) ( ops_controller_taskrun_info{namespace=\"$namespace\"} == 1 * on(namespace, name) group_left(status) ops_controller_taskrun_status{namespace=\"$namespace\"} ) 获取 Host 列表及状态 ops_controller_host_info 按关键字统计 EventHooks 触发次数 count by (name, keyword) (ops_controller_eventhooks_status{namespace=\"$namespace\",keyword!=\"\"}) 获取 TaskRun 运行时长 ops_controller_taskrun_duration_seconds 获取 Controller CPU 使用率 rate(ops_controller_resource_cpu_usage_seconds_total{pod=\"xxx\"}[5m]) 获取 Controller 内存使用量 ops_controller_resource_memory_usage_bytes{pod=\"xxx\"} 获取 Server CPU 使用率 rate(ops_server_resource_cpu_usage_seconds_total{pod=\"xxx\"}[5m]) 获取 Server 内存使用量 ops_server_resource_memory_usage_bytes{pod=\"xxx\"} "},"opscontroller-events.html":{"url":"opscontroller-events.html","title":"Events","keywords":"","body":"Ops 事件系统 概述 Ops 项目通过 NATS JetStream 发布各种事件，用于监控、告警和集成。所有事件都遵循 CloudEvents 标准，并通过 NATS 主题（Subject）进行路由。 事件主题格式 标准格式 大多数事件主题遵循以下格式： ops.clusters.{cluster}.namespaces.{namespace}.{resourceType}.{resourceName}.{observation} 其中： {cluster}: 集群名称（从环境变量 EVENT_CLUSTER 获取） {namespace}: Kubernetes 命名空间 {resourceType}: 资源类型（如 hosts, clusters, taskruns 等） {resourceName}: 资源名称 {observation}: 观测类型，可能的值及含义： status: 资源状态信息（如运行状态、健康状态等） metrics: 指标数据（如 CPU、内存、磁盘使用率等） logs: 日志信息 events: 事件信息（如 Kubernetes 事件） traces: 追踪信息（如分布式追踪数据） alerts: 告警信息 findings: 主动上报的信息和状态 节点事件特殊格式 节点（Node）事件使用特殊格式，不包含 namespaces 部分： ops.clusters.{cluster}.nodes.{nodeName}.{observation} 示例： ops.clusters.mycluster.nodes.mynode.events 说明： 节点是集群级别的资源，不属于任何命名空间 节点事件的主题格式省略了 namespaces 部分 其他格式规则与标准格式相同 事件类型列表 1. Controller 设置事件 主题格式： ops.clusters.{cluster}.namespaces.{namespace}.controllers.{resourceType}.status 触发时机： 各个 Controller 启动时发布，表示该 Controller 已就绪 发布位置： controllers/host_controller.go: Host Controller 启动 controllers/cluster_controller.go: Cluster Controller 启动 controllers/task_controller.go: Task Controller 启动 controllers/taskrun_controller.go: TaskRun Controller 启动 controllers/pipeline_controller.go: Pipeline Controller 启动 controllers/pipelinerun_controller.go: PipelineRun Controller 启动 controllers/event_controller.go: Event Controller 启动 controllers/eventhooks_controller.go: EventHook Controller 启动 事件数据结构： { \"cluster\": \"string\", \"kind\": \"string\" // 资源类型，如 \"Hosts\", \"Clusters\", \"TaskRuns\" 等 } 2. Host 状态事件 主题格式： ops.clusters.{cluster}.namespaces.{namespace}.hosts.{hostName}.status 触发时机： Host 资源状态更新时（心跳状态、磁盘使用率等） 发布位置： controllers/host_controller.go: Host 状态变更时 事件数据结构： { \"cluster\": \"string\", \"address\": \"string\", \"port\": 0, \"username\": \"string\", \"status\": { \"hostname\": \"string\", \"diskUsagePercent\": \"string\", \"heartStatus\": \"string\", \"heartTime\": \"string\", // ... 其他 HostStatus 字段 } } 3. Cluster 状态事件 主题格式： ops.clusters.{cluster}.namespaces.{namespace}.clusters.{clusterName}.status 触发时机： Cluster 资源状态更新时（Kubernetes 版本、证书过期时间、心跳状态等） 发布位置： controllers/cluster_controller.go: Cluster 状态变更时 事件数据结构： { \"cluster\": \"string\", \"server\": \"string\", \"status\": { \"version\": \"string\", \"certNotAfterDays\": 0, \"heartStatus\": \"string\", \"heartTime\": \"string\", // ... 其他 ClusterStatus 字段 } } 4. TaskRun 状态事件 主题格式： ops.clusters.{cluster}.namespaces.{namespace}.taskruns.{taskRunName}.status 触发时机： TaskRun 执行完成或状态变更时 发布位置： controllers/taskrun_controller.go: TaskRun 执行完成时（run 函数中） TaskRun 状态变更时（Reconcile 中） 事件数据结构： { \"cluster\": \"string\", \"taskRef\": \"string\", \"desc\": \"string\", \"variables\": { \"key\": \"value\" }, \"runStatus\": \"string\", \"startTime\": \"string\", \"taskRunNodeStatus\": [ { \"nodeName\": \"string\", \"runStatus\": \"string\", \"taskRunStep\": [ { \"stepName\": \"string\", \"stepContent\": \"string\", \"stepOutput\": \"string\", \"stepStatus\": \"string\" } ] } ] } 5. PipelineRun 状态事件 主题格式： ops.clusters.{cluster}.namespaces.{namespace}.pipelineruns.{pipelineRunName}.status 触发时机： PipelineRun 执行完成或状态变更时 跨集群 PipelineRun 状态同步完成时 发布位置： controllers/pipelinerun_controller.go: PipelineRun 执行完成时（run 函数中） PipelineRun 状态变更时（Reconcile 中） 跨集群 PipelineRun 状态同步完成时（goroutine 中） 事件数据结构： { \"cluster\": \"string\", \"pipelineRef\": \"string\", \"desc\": \"string\", \"variables\": { \"key\": \"value\" }, \"runStatus\": \"string\", \"startTime\": \"string\", \"pipelineRunStatus\": [ { \"taskName\": \"string\", \"taskRef\": \"string\", \"taskRunStatus\": { \"runStatus\": \"string\", \"taskRunNodeStatus\": [] } } ] } 6. Kubernetes 事件 主题格式： 对于命名空间资源： ops.clusters.{cluster}.namespaces.{namespace}.{resourceKind}s.{resourceName}.events 对于节点（Node）资源（特殊格式，无 namespaces）： ops.clusters.{cluster}.nodes.{nodeName}.events 触发时机： Kubernetes 原生事件（events.k8s.io/v1）的创建、更新、删除时 仅发布最近 2 分钟内的事件 发布位置： controllers/event_controller.go: 监听 Kubernetes Event 资源 事件数据结构： { \"cluster\": \"string\", \"type\": \"string\", // Normal, Warning 等 \"reason\": \"string\", // 事件原因 \"eventTime\": \"string\", // ISO 8601 格式 \"from\": \"string\", // 事件来源（Manager） \"message\": \"string\" // 事件消息 } 示例主题： 命名空间资源： ops.clusters.mycluster.namespaces.ops-system.pods.my-pod.events ops.clusters.mycluster.namespaces.ops-system.deployments.my-deployment.events 节点资源（特殊格式）： ops.clusters.mycluster.nodes.mynode.events 7. 通知事件 主题格式： ops.notifications.providers.{provider}.channels.{channel}.severities.{severity} 其中： {provider}: 通知提供商或系统名称（如 ksyun, ai 等） {channel}: 通知渠道类型（如 webhook, email, sms 等） {severity}: 严重程度级别（如 info, warning, error, critical 等） 触发时机： 通知系统发送通知时发布 发布位置： 通过 API 接口 /api/v1/namespaces/{namespace}/events/{event} 发布 如果事件路径以 ops. 开头，将直接使用该路径作为主题，不进行格式转换 发布方式： curl -X POST http://localhost:80/api/v1/namespaces/ops-system/events/ops.notifications.providers.ksyun.channels.webhook.severities.info \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer YOUR_TOKEN\" \\ -d '{ \"provider\": \"ksyun\", \"channel\": \"webhook\", \"severity\": \"info\", \"message\": \"Notification message\", \"title\": \"Notification title\" }' 事件数据结构： { \"provider\": \"string\", \"channel\": \"string\", \"severity\": \"string\", \"message\": \"string\", \"title\": \"string\", \"timestamp\": \"string\", // ... 其他通知相关字段 } 示例主题： ops.notifications.providers.ksyun.channels.webhook.severities.info ops.notifications.providers.ksyun.channels.webhook.severities.warning ops.notifications.providers.ksyun.channels.webhook.severities.error ops.notifications.providers.ksyun.channels.email.severities.critical ops.notifications.providers.AI.channels.webhook.severities.info 说明： 通知事件使用独立的主题格式，不包含 clusters 和 namespaces 部分 用于通知系统的路由和分发 支持多个提供商、渠道和严重程度级别的组合 重要：如果事件路径以 ops. 开头，API 会直接使用该路径作为 NATS 主题，不会进行任何格式转换 8. 自定义事件（通过 API） 主题格式： 根据事件路径的不同，API 会采用不同的处理方式： 以 ops. 开头的事件路径（直接投递）： ops.{任意路径} 直接使用该路径作为 NATS 主题，不进行任何格式转换 适用于通知事件等独立格式的事件 以 nodes. 开头的事件路径（节点事件）： ops.clusters.{cluster}.nodes.{nodeName}.{observation} 转换为节点事件格式，不包含 namespaces 部分 标准格式（其他路径）： ops.clusters.{cluster}.namespaces.{namespace}.{eventName} 自动添加集群和命名空间前缀 触发时机： 通过 API 接口 /api/v1/namespaces/{namespace}/events/{event} 手动发布 发布位置： pkg/server/api.go: CreateEvent 函数 事件数据结构： 由调用方自定义，可以是任意 JSON 对象 使用示例： 标准格式事件： curl -X POST http://localhost:80/api/v1/namespaces/ops-system/events/my-custom-event \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer YOUR_TOKEN\" \\ -d '{ \"message\": \"Custom event data\", \"level\": \"info\" }' 直接投递格式（以 ops. 开头）： curl -X POST http://localhost:80/api/v1/namespaces/ops-system/events/ops.notifications.providers.ksyun.channels.webhook.severities.info \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer YOUR_TOKEN\" \\ -d '{ \"provider\": \"ksyun\", \"channel\": \"webhook\", \"severity\": \"info\", \"message\": \"Notification message\" }' 节点事件格式： curl -X POST http://localhost:80/api/v1/namespaces/ops-system/events/nodes.mynode.findings \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer YOUR_TOKEN\" \\ -d '{ \"message\": \"Node finding\", \"status\": \"normal\" }' 事件订阅示例 订阅所有事件 nats --user=app --password=${apppassword} sub \"ops.>\" 订阅特定命名空间的事件 nats --user=app --password=${apppassword} sub \"ops.clusters.*.namespaces.ops-system.>\" 订阅 Host 状态事件 nats --user=app --password=${apppassword} sub \"ops.clusters.*.namespaces.*.hosts.*.status\" 订阅 TaskRun 状态事件 nats --user=app --password=${apppassword} sub \"ops.clusters.*.namespaces.*.taskruns.*.status\" 订阅 PipelineRun 状态事件 nats --user=app --password=${apppassword} sub \"ops.clusters.*.namespaces.*.pipelineruns.*.status\" 订阅 Kubernetes 事件 # 订阅所有命名空间资源的 Kubernetes 事件 nats --user=app --password=${apppassword} sub \"ops.clusters.*.namespaces.*.*.events\" # 订阅节点事件（特殊格式） nats --user=app --password=${apppassword} sub \"ops.clusters.*.nodes.*.events\" 订阅通知事件 # 订阅所有通知事件 nats --user=app --password=${apppassword} sub \"ops.notifications.>\" # 订阅特定提供商的通知事件 nats --user=app --password=${apppassword} sub \"ops.notifications.providers.ksyun.>\" # 订阅特定渠道的通知事件 nats --user=app --password=${apppassword} sub \"ops.notifications.providers.*.channels.webhook.>\" # 订阅特定严重程度的通知事件 nats --user=app --password=${apppassword} sub \"ops.notifications.providers.*.channels.*.severities.error\" 事件格式说明 所有事件都遵循 CloudEvents 标准，包含以下标准字段： id: 事件唯一标识符（UUID） source: 事件来源（固定为 https://github.com/shaowenchen/ops） type: 事件类型（如 Host, Cluster, PipelineRun 等） specversion: CloudEvents 规范版本（1.0） time: 事件时间戳（ISO 8601 格式） data: 事件数据（JSON 格式） subject: NATS 主题（自动设置） 环境变量配置 事件系统依赖以下环境变量： EVENT_CLUSTER: 集群名称，用于构建事件主题 EVENT_ENDPOINT: NATS 服务器地址（如 nats://nats:4222） 注意事项 事件去重: NATS JetStream 支持消息去重，通过 dupe-window 配置（默认 2 分钟） 事件保留: 根据 Stream 配置决定事件保留时间（默认 24 小时） 事件顺序: 同一主题的事件按时间顺序发布 异步发布: 大部分事件通过 goroutine 异步发布，不会阻塞主流程 事件过滤: Kubernetes 事件仅发布最近 2 分钟内的事件，避免历史事件干扰 相关文档 NATS 配置文档 EventHooks 使用文档 "},"opscontroller-eventhooks.html":{"url":"opscontroller-eventhooks.html","title":"EventHooks","keywords":"","body":"EventHooks 使用指南 概述 EventHooks 是 Ops 提供的事件通知机制，允许您根据事件的关键词匹配规则，将事件转发到不同的通知渠道。支持多种通知类型，包括 Webhook、协作文档、事件转发和 Elasticsearch。 基本概念 EventHooks 资源 EventHooks 是一个 Kubernetes CRD 资源，用于定义事件通知规则： apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: my-eventhook namespace: ops-system spec: type: webhook # 通知类型 subject: ops.clusters.*.namespaces.*.pods.*.events # 订阅的事件主题 url: https://example.com/webhook # 通知目标 URL keywords: # 关键词匹配规则 include: - \"Error\" matchType: CONTAINS 字段说明 type: 通知类型，支持的值： webhook: HTTP Webhook 通知 xiezuo: 协作文档通知 event: 事件转发（转发到 NATS 主题） elasticsearch: Elasticsearch 索引 subject: 订阅的事件主题，支持通配符（如 ops.clusters.*.namespaces.*.pods.*.events） url: 通知目标地址，根据不同的通知类型有不同的格式要求 keywords: 关键词匹配配置 options: 额外的配置选项，不同通知类型支持不同的选项 关键词匹配 匹配模式 (matchMode) ANY（默认）: 只要包含列表中的任意一个关键词即匹配 ALL: 必须包含列表中的所有关键词才匹配 匹配类型 (matchType) CONTAINS（默认）: 字符串包含匹配 EXACT: 精确匹配 REGEX: 正则表达式匹配 示例 示例 1: 包含匹配 keywords: include: - \"Error\" - \"Failed\" matchMode: ANY matchType: CONTAINS 匹配包含 \"Error\" 或 \"Failed\" 的事件。 示例 2: 正则表达式匹配 keywords: include: - \"(?=.*(kube|etcd|calico|csi|fluid)).*(BackOff|OOMKilled|Evicted|NetworkNotReady|Unhealthy|Error|Failed|ImagePullBackOff).*\" matchType: REGEX 使用正则表达式匹配： 必须包含：kube、etcd、calico、csi 或 fluid 中的任意一个 且必须包含：BackOff、OOMKilled、Evicted 等关键词中的任意一个 示例 3: 排除匹配 keywords: include: - \"Error\" exclude: - \"healthcheck\" - \"test\" matchType: CONTAINS 匹配包含 \"Error\" 但不包含 \"healthcheck\" 或 \"test\" 的事件。 通知类型详解 1. Webhook 通知 (webhook) 将事件数据以 JSON 格式发送到指定的 HTTP Webhook URL。 配置示例： apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: webhook-notification namespace: ops-system spec: type: webhook subject: ops.clusters.*.namespaces.*.pods.*.events url: https://example.com/api/webhook keywords: include: - \"Error\" matchType: CONTAINS 发送的数据格式： 事件的可读字符串格式（包含事件的所有字段信息）。 2. 协作文档通知 (xiezuo) 发送到协作文档系统的 Webhook。 配置示例： apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: xiezuo-notification namespace: ops-system spec: type: xiezuo subject: ops.clusters.*.namespaces.*.pods.*.events url: https://365.kdocs.cn/woa/api/v1/webhook/send?key=xxxx keywords: include: - \"Error\" matchType: CONTAINS 数据格式： 如果数据已经是 XiezuoBody 格式，直接发送；否则转换为文本格式发送。 3. 事件转发 (event) 将事件转发到另一个 NATS 主题。支持通配符替换。 配置示例： apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: event-forward namespace: ops-system spec: type: event subject: ops.clusters.*.namespaces.*.pods.*.events # URL 中使用通配符 *，会自动替换为原始事件 subject 中对应位置的值 url: ops.clusters.*.namespaces.*.pods.*.alerts keywords: include: - \"(?=.*(kube|etcd|calico)).*(BackOff|OOMKilled|Evicted|Error|Failed).*\" matchType: REGEX 通配符替换示例： 原始事件 subject: ops.clusters.cluster1.namespaces.ns1.pods.pod1.events URL 模板: ops.clusters.*.namespaces.*.pods.*.alerts 替换后: ops.clusters.cluster1.namespaces.ns1.pods.pod1.alerts 节点事件支持： 也支持节点事件格式： 原始事件: ops.clusters.cluster1.nodes.node1.events URL 模板: ops.clusters.*.nodes.*.alerts 替换后: ops.clusters.cluster1.nodes.node1.alerts 注意事项： 转发的事件会生成新的 ID 和时间戳 需要设置环境变量 EVENT_ENDPOINT 指定 NATS 服务器地址 4. Elasticsearch 通知 (elasticsearch) 将事件数据索引到 Elasticsearch。 配置示例： apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: elasticsearch-notification namespace: ops-system spec: type: elasticsearch subject: ops.clusters.*.namespaces.*.pods.*.events url: http://elasticsearch:9200/ops-events/_doc options: username: elastic # Elasticsearch 用户名（可选） password: changeme # Elasticsearch 密码（可选） index: ops-events # 覆盖 URL 中的索引名（可选） id: \"\" # 文档 ID（可选，不指定则自动生成） keywords: include: - \"Error\" matchType: CONTAINS URL 格式： http://host:port/index/_doc - 自动生成文档 ID http://host:port/index/_doc/doc-id - 指定文档 ID http://host:port - 需要在 options 中指定 index 基于日期的索引命名： 可以在索引名中使用日期占位符来创建基于时间的索引。支持的占位符： {date} 或 {YYYY.MM.DD} -> 2024.01.19 {YYYY-MM-DD} -> 2024-01-19 {YYYYMMDD} -> 20240119 {YYYY.MM} -> 2024.01 (月度索引) {YYYY-MM} -> 2024-01 (月度索引) {YYYY} -> 2024 (年度索引) 使用日期索引的示例： spec: type: elasticsearch subject: ops.clusters.*.namespaces.*.pods.*.events url: http://elasticsearch:9200 options: index: ops-events-{YYYY.MM.DD} # 创建类似 ops-events-2024.01.19 的索引 username: elastic password: changeme 这将创建按日期命名的索引，例如： ops-events-2024.01.19 ops-events-2024.01.20 等等 索引的文档结构： 索引的文档只包含来自 event.Data() 的原始事件数据，不添加任何额外的元数据或扩展字段。 示例： 如果原始事件数据是： { \"cluster\": \"cluster1\", \"namespace\": \"ns1\", \"pod\": \"pod1\", \"type\": \"Warning\", \"reason\": \"BackOff\", \"message\": \"事件消息\" } 那么索引的文档将完全相同： { \"cluster\": \"cluster1\", \"namespace\": \"ns1\", \"pod\": \"pod1\", \"type\": \"Warning\", \"reason\": \"BackOff\", \"message\": \"事件消息\" } 注意： 只索引原始事件数据。不会添加任何元数据字段（如 @timestamp、event_id、event_type 等）或扩展字段（如 ext_*）。 支持的选项： username: Elasticsearch 用户名（用于 Basic 认证） password: Elasticsearch 密码（用于 Basic 认证） index: 覆盖 URL 中的索引名（支持日期占位符，如 {YYYY.MM.DD}） id: 文档 ID（如果 URL 中未指定） 完整示例 示例 1: 将 Pod 错误事件发送到 Webhook apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: pod-errors-to-webhook namespace: ops-system spec: type: webhook subject: ops.clusters.*.namespaces.*.pods.*.events url: https://alerting.example.com/webhook keywords: include: - \"(?=.*(kube|etcd|calico|csi|fluid)).*(BackOff|OOMKilled|Evicted|NetworkNotReady|Unhealthy|Error|Failed|ImagePullBackOff).*\" matchType: REGEX 示例 2: 将事件转发到告警主题 apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: convert-events-to-alerts namespace: ops-system spec: type: event subject: ops.clusters.*.namespaces.*.pods.*.events url: ops.clusters.*.namespaces.*.pods.*.alerts keywords: include: - \"(?=.*(kube|etcd|calico|csi|fluid)).*(BackOff|OOMKilled|Evicted|NetworkNotReady|Unhealthy|Error|Failed|ImagePullBackOff).*\" matchType: REGEX 示例 3: 将事件索引到 Elasticsearch apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: events-to-elasticsearch namespace: ops-system spec: type: elasticsearch subject: ops.clusters.*.namespaces.*.pods.*.events url: http://elasticsearch:9200/ops-events/_doc options: username: elastic password: changeme keywords: include: - \"Error\" matchType: CONTAINS 示例 3b: 使用日期索引将事件索引到 Elasticsearch apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: events-to-elasticsearch-daily namespace: ops-system spec: type: elasticsearch subject: ops.clusters.*.namespaces.*.pods.*.events url: http://elasticsearch:9200 options: index: ops-events-{YYYY.MM.DD} # 创建按日期命名的索引，如 ops-events-2024.01.19 username: elastic password: changeme keywords: include: - \"Error\" matchType: CONTAINS 示例 4: 节点事件处理 apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: node-events-to-alerts namespace: ops-system spec: type: event subject: ops.clusters.*.nodes.*.events url: ops.clusters.*.nodes.*.alerts keywords: include: - \"(?=.*(kube|etcd|calico)).*(BackOff|OOMKilled|Error|Failed).*\" matchType: REGEX 最佳实践 使用正则表达式进行复杂匹配：当需要匹配多个条件时，使用正则表达式可以更精确地控制匹配规则。 合理使用排除规则：使用 exclude 可以过滤掉不需要的事件，减少误报。 事件转发使用通配符：在事件转发场景中，使用通配符可以保持事件的主题结构，便于后续处理。 Elasticsearch 索引命名：建议使用有意义的索引名，可以考虑按日期或集群名称组织索引。 监控 EventHooks 状态：通过 Prometheus 指标监控 EventHooks 的触发情况，及时发现配置问题。 故障排查 查看 EventHooks 状态 kubectl get eventhooks -n ops-system kubectl describe eventhooks -n ops-system 查看 Controller 日志 kubectl logs -n ops-system deployment/ops-controller-manager | grep eventhook 检查指标 # 查看 EventHooks 触发次数 curl http://localhost:8080/metrics | grep ops_controller_eventhooks_status 常见问题 事件未触发： 检查 subject 是否正确匹配事件主题 检查 keywords 配置是否正确 查看 Controller 日志确认是否有错误 Webhook 发送失败： 检查 URL 是否可访问 检查网络连接 查看 Controller 日志中的错误信息 Elasticsearch 索引失败： 检查 Elasticsearch 连接 验证认证信息 检查索引权限 相关文档 事件系统文档 指标文档 "},"nats.html":{"url":"nats.html","title":"Nats","keywords":"","body":"Nats 用途 Ops 通过 Nats 组件，导出相关的事件，主要有两类: CRD 的状态，包括主机、集群的状态，TaskRun、PipelineRun 的状态 alert 定时巡检上报的状态信息 下面提供 Nats 组件的安装与配置。这里采用的是，一个主集群，若干边缘集群的方式，边缘集群会将事件转发到主集群，在主集群统一进行处理。 添加 Helm Repo 添加仓库 helm repo add nats https://nats-io.github.io/k8s/helm/charts/ helm repo update 查看可配置的字段 helm show values nats/nats 部署主集群 设置 Nats 的基本信息 export adminpassword=mypassword export apppassword=mypassword 生成 nats-values.yaml cat nats-values.yaml config: jetstream: enabled: true fileStore: enabled: false dir: /data memoryStore: enabled: true maxSize: 1Gi pvc: enabled: false storageClassName: my-sc cluster: enabled: true leafnodes: enabled: true merge: accounts: SYS: users: - user: admin password: ${adminpassword} APP: users: - user: app password: ${apppassword} jetstream: true system_account: SYS container: image: repository: nats tag: 2.10.20-alpine natsBox: container: image: repository: nats-box tag: 0.14.5 reloader: enabled: true image: repository: natsio/nats-server-config-reloader tag: 0.15.1 EOF 数据被持久化到内存中，如果需要存储到磁盘，需要设置 fileStore。 安装 nats helm -n ops-system install nats nats/nats --version 1.2.4 -f nats-values.yaml helm -n ops-system uninstall nats 暴露 Nats 服务端口 kubectl patch svc nats -p '{\"spec\":{\"type\":\"NodePort\",\"ports\":[{\"port\":4222,\"nodePort\":32223,\"targetPort\":\"nats\"},{\"port\":7422,\"nodePort\":32222,\"targetPort\":\"leafnodes\"}]}}' -n ops-system 查看负载 kubectl -n ops-system get pod,svc | grep nats pod/nats-0 2/2 Running 0 15h pod/nats-1 2/2 Running 0 15h pod/nats-2 2/2 Running 0 15h pod/nats-box-6bb86df889-xcr6x 1/1 Running 0 15h service/nats NodePort 10.100.109.24 4222:32223/TCP,7422:32222/TCP 15h service/nats-headless ClusterIP None 4222/TCP,7422/TCP,6222/TCP,8222/TCP 15h 部署边缘节点 添加仓库 helm repo add nats https://nats-io.github.io/k8s/helm/charts/ helm repo update 设置主集群的 nats 信息 export natsendpoint=x.x.x.x:32222 设置 nats 的 server_name export natsservername=need-to-be-unique 生成 nats-values.yaml 需要注意的是，不同集群的 server_name 不能相同，否则会有重复连接的问题。 cat nats-values.yaml config: leafnodes: enabled: true merge: remotes: - urls: - nats://admin:${adminpassword}@${natsendpoint} account: SYS - urls: - nats://app:${apppassword}@${natsendpoint} account: APP merge: server_name: ${natsservername} accounts: SYS: users: - user: admin password: ${adminpassword} APP: users: - user: app password: ${apppassword} jetstream: true system_account: SYS container: image: repository: nats tag: 2.10.20-alpine natsBox: container: image: repository: natsio/nats-box tag: 0.14.5 reloader: enabled: true image: repository: natsio/nats-server-config-reloader tag: 0.15.1 EOF 安装 nats helm install nats nats/nats --version 1.2.4 -f nats-values.yaml -n ops-system Nats 常用命令 测试 Nats kubectl -n ops-system exec -it deployment/nats-box -- sh 订阅消息 nats --user=app --password=${apppassword} sub \"ops.>\" 发布消息 nats --user=app --password=${apppassword} pub ops.test \"mymessage mycontent\" 删除 stream nats --user=app --password=${apppassword} stream rm ops 创建 stream 持久化消息 nats --user=app --password=${apppassword} stream add ops --subjects \"ops.>\" --ack --max-msgs=-1 --max-bytes=-1 --max-age=24h --storage file --retention limits --max-msg-size=-1 --discard=old --replicas 1 --dupe-window=2m 生产环境中，推荐使用 file 存储，并且 replica 设置为 3。 查看 stream 事件 nats --user=app --password=${apppassword} stream view ops 查看 stream 配置 nats --user=app --password=${apppassword} stream info ops 查看集群信息 nats --user=admin --password=${adminpassword} server report jetstream 这里可以看到，主集群的信息，边缘集群的信息，以及连接的信息。 查看 stream 的 subjects nats --user=app --password=${apppassword} stream subjects ops 压力测试 nats --user=app --password=${apppassword} bench benchsubject --pub 1 --sub 10 参考 JetStream 配置 LeafNode 配置 Gateway 配置 "}}
{"./":{"url":"./","title":"Introduction","keywords":"","body":"Ops Overview Ops is an operations tool designed to provide a simple and efficient platform for system administrators to complete maintenance tasks quickly. It aims to streamline operations, with a focus on automation and task management across multiple systems and clusters. Production Use Cases CICD Clusters: Building 2k+ daily for CICD clusters. Global Clusters: Managing 40+ clusters overseas. AI Computing Clusters: Managing 5+ AI computing clusters. Architecture Support: Supporting both ARM and X86 architectures. Design Overview The core components of Ops are built around objects that represent hosts, clusters, and tasks. The design allows for efficient orchestration of operations in a Kubernetes-native environment. Key Objects: Host: Represents machines (cloud-based or bare-metal) that can be accessed via SSH. Cluster: Represents Kubernetes clusters that can be accessed via kubectl. Task: Represents a combination of multiple files and shell commands. Pipeline: Represents a sequence of tasks executed in a specific order. Core Operations: File: Uploading and distributing files to hosts and clusters. Shell: Executing shell scripts on remote hosts or clusters. Components ops-cli: A command-line tool that assists system administrators with automation tasks. ops-server: An HTTP service that provides RESTful APIs and a Dashboard interface for managing and monitoring tasks and resources. ops-controller: A Kubernetes Operator that manages hosts, clusters, tasks, pipelines, and other resources in the Kubernetes environment. Multi-Cluster Support In practice, it is recommended to: Host Creation: Create hosts based on the current clusterâ€™s machines. Cluster Creation: Multiple clusters can be added, each cluster will be treated as a managed cluster. Task and Pipeline objects are automatically synchronized across all clusters under management, removing the need for manual triggering. When deploying a pipeline, a PipelineRun object is created, which can span multiple clusters. Unlike TaskRuns, PipelineRuns can cross clusters. The ops-controller watches the PipelineRun object, and based on the cluster field, it dispatches the pipeline to the corresponding cluster's controller, which executes the tasks and updates the status of the PipelineRun. Event-Driven Architecture Ops adopts an event-driven approach to manage operations: Heartbeat Events: These events are triggered periodically to check the health of hosts and clusters. Task Execution Events: Events that are triggered when a TaskRun or PipelineRun task is executed. Inspection Events: Triggered during scheduled tasks or inspections to monitor the system. Webhook Events: Custom events like alerts and notifications for maintenance activities. Event Aggregation in Multi-Cluster Setup: In a multi-cluster environment, each cluster is recommended to install a Nats component to collect events from edge clusters. Events are then aggregated into one or more clusters to centralize the monitoring and management of operational tasks. System Architecture The overall architecture is built to handle large-scale, multi-cluster environments, ensuring that tasks and pipelines can be seamlessly distributed and monitored across multiple regions and systems. The ops-server acts as a central hub for managing tasks and pipelines, while the ops-controller manages resources in the Kubernetes clusters. This system is designed to provide comprehensive and automated operations management, facilitating the handling of complex tasks across diverse infrastructures. "},"opscli.html":{"url":"opscli.html","title":"Opscli","keywords":"","body":"opscli Overview opscli is a command-line interface (CLI) designed to facilitate batch remote execution of commands, file distribution, and management of Ops Controller CRD resources. It primarily deals with three types of CRDs: Host, Cluster, and Task. Main Features of opscli Batch Remote Command ExecutionAllows you to run commands across multiple remote hosts or nodes in a cluster. Batch File DistributionEnables bulk file distribution to remote systems. Ops Controller CRD ResourcesFacilitates the creation and management of Host, Cluster, and Task resources in the Ops Controller. Supported Operating Systems Linux macOS "},"opscli-install.html":{"url":"opscli-install.html","title":"Install","keywords":"","body":"opscli Installation Guide 1. Single Machine Installation For Domestic Users (China) PROXY=https://ghfast.top/ curl -sfL $PROXY/https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh | VERSION=latest PROXY=$PROXY sh - For International Users (Outside China) curl -sfL https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh | VERSION=latest sh - 2. Batch Installation To install opscli on multiple hosts, list the IP addresses of all hosts in a hosts.txt file, and use the opscli shell command to execute the installation. The default credential is the current user's ~/.ssh/id_rsa. For Domestic Users (China) /usr/local/bin/opscli shell --content \"curl -sfL https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh | VERSION=latest sh -\" -i hosts.txt For International Users (Outside China) /usr/local/bin/opscli shell --content \"curl -sfL https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh | VERSION=latest sh -\" -i hosts.txt 3. Version Upgrade For Single Machine sudo /usr/local/bin/opscli upgrade For Batch Upgrade /usr/local/bin/opscli shell --content \"sudo /usr/local/bin/opscli upgrade\" -i hosts.txt 4. Auto-completion Setup For bash echo 'source >~/.bashrc For zsh echo 'source >~/.zshrc 5. More Information To see additional usage options: /usr/local/bin/opscli --help "},"opscli-shell.html":{"url":"opscli-shell.html","title":"Shell","keywords":"","body":"opscli Shell Command Guide 1. Specify Target Hosts Single Host Use the -i flag to specify a single host IP. -i 1.1.1.1 You can also specify a username and password with the --username and --password flags: --username --password Batch Hosts To specify multiple hosts, you can use a file or comma-separated IP addresses. From a File (hosts.txt) -i hosts.txt Example content of hosts.txt: 1.1.1.1 2.2.2.2 Comma-separated IPs -i 1.1.1.1,2.2.2.2 All Nodes in a Cluster -i ~/.kube/config --nodename all By default, -i points to ~/.kube/config. Specific Node in a Cluster -i ~/.kube/config --nodename node1 Where node1 is the node name. 2. View Cluster Images For Single Machine /usr/local/bin/opscli task -f ~/.ops/tasks/list-podimage.yaml --namespace all 3. Cluster Bulk Operations All Nodes To run a command on all nodes: opscli shell --content \"uname -a\" --nodename all Specific Node To run a command on a specific node: opscli shell --content \"uname -a\" --nodename node1 Specify kubeconfig To specify a custom kubeconfig, use the -i flag: opscli shell -i ~/Documents/opscli/prod --content \"uname -a\" --nodename node1 "},"opscli-task.html":{"url":"opscli-task.html","title":"Task","keywords":"","body":"opscli Task Command Guide 1. Specify Target Hosts Single Host -i 1.1.1.1 You can specify the username and password for the host with --username and --password. Batch Hosts To specify multiple hosts via a file: -i hosts.txt Example content of hosts.txt: 1.1.1.1 2.2.2.2 All Nodes in a Cluster -i ~/.kube/config --nodename all Specific Node in a Cluster -i ~/.kube/config --nodename node1 Where node1 is the node name. 2. Update /etc/hosts On a Single Host To remotely update the /etc/hosts file on host 1.1.1.1: /usr/local/bin/opscli task -f ~/.ops/tasks/set-hosts.yaml --ip 1.2.3.4 --domain test.com --i 1.1.1.1 --port 2222 --username root To clear the /etc/hosts entry, add --clear: /usr/local/bin/opscli task -f ~/.ops/tasks/set-hosts.yaml --ip 1.2.3.4 --domain test.com --i 1.1.1.1 --clear On All Nodes in a Cluster /usr/local/bin/opscli task -f ~/.ops/tasks/set-hosts.yaml --ip 1.2.3.4 --domain test.com --i ~/.kube/config --nodename all On a Specific Node in a Cluster /usr/local/bin/opscli task -f ~/.ops/tasks/set-hosts.yaml --ip 1.2.3.4 --domain test.com --i ~/.kube/config --nodename node1 3. Install Applications Install Istio /usr/local/bin/opscli task -f ~/.ops/tasks/app-istio.yaml --version 1.13.7 --kubeconfig /etc/kubernetes/admin.conf The default version is 1.13.7 and the default kubeconfig is /etc/kubernetes/admin.conf. Uninstall Istio /usr/local/bin/opscli task -f ~/.ops/tasks/app-istio.yaml --version 1.13.7 --kubeconfig /etc/kubernetes/admin.conf --action delete 4. Upload Files Upload to Server /usr/local/bin/opscli task -f tasks/file-upload.yaml --api https://gh-uploadapi.chenshaowen.com/api/v1/files --localfile dockerfile Upload to S3 /usr/local/bin/opscli task -f tasks/file-upload.yaml --ak xxx --sk xxx --region beijing --endpoint ks3-cn-beijing.ksyun.com --bucket xxx --localfile dockerfile --remotefile s3://dockerfile 5. Download Files From Server /usr/local/bin/opscli task -f tasks/file-download.yaml --api https://gh-uploadapi.chenshaowen.com/api/v1/files --aeskey --remotefile --localfile dockerfile1 From S3 /usr/local/bin/opscli task -f tasks/file-download.yaml --ak xxx --sk xxx --region beijing --endpoint ks3-cn-beijing.ksyun.com --bucket xxx --localfile dockerfile2 --remotefile s3://dockerfile "},"opscli-file.html":{"url":"opscli-file.html","title":"File","keywords":"","body":"opscli file Command Usage The opscli file command is used for transferring files between the local host, object storage, API servers, and clusters. Below are the details for various use cases. 1. Host - Local and Object Storage File Transfer Set AK/SK (Access Key / Secret Key) export ak= export sk= Upload a Local File to Object Storage To upload a file ./tmp.log to the object storage at s3://logs/tmp.log: /usr/local/bin/opscli file --direction upload --localfile ./tmp.log --remotefile s3://logs/tmp.log --bucket obs-test Here: --bucket is the S3 bucket name. --region is the S3 bucket's region. --endpoint is the S3 bucket's endpoint. --direction specifies the upload direction. --localfile is the local file to be uploaded. --remotefile is the destination file in object storage. Download a File from S3 to Local To download s3://logs/tmp.log to the local file ./tmp1.log: /usr/local/bin/opscli file --direction download --localfile ./tmp1.log --remotefile s3://logs/tmp.log --bucket obs-test Unset AK/SK To clear the AK/SK environment variables: unset ak unset sk 2. Host - Local and API Server File Transfer This option provides encryption/decryption for file transfers with the API server. Upload to API Server /usr/local/bin/opscli file --direction upload --api https://gh-uploadapi.chenshaowen.com/api/v1/files --localfile ./tmp.log If aeskey is \"\", a random encryption key is generated automatically. If not set, the file is uploaded without encryption. Download from API Server /usr/local/bin/opscli file --api https://gh-uploadapi.chenshaowen.com/api/v1/files --aeskey xxx --direction download --remotefile https://download_url_link.com.aes 3. Cluster - Local and API Server File Transfer Upload to Cluster's API Server /usr/local/bin/opscli file -i ~/.kube/config --nodename node1 --direction upload --api https://gh-uploadapi.chenshaowen.com/api/v1/files --aeskey \"\" --localfile /root/tmp.log --runtimeimage shaowenchen/ops-cli Download from Cluster's API Server /usr/local/bin/opscli file -i ~/.kube/config --nodename xxx --direction download --api https://gh-uploadapi.chenshaowen.com/api/v1/files --aeskey xxx --localfile /root/tmp1.log --remotefile https://gh-uploadapi.chenshaowen.com/uploadbases/cdn0/raw/1721621949-tmp.log.aes --runtimeimage shaowenchen/ops-cli 4. Cluster - Local and Object Storage File Transfer Upload to Object Storage from Cluster /usr/local/bin/opscli file -i ~/.kube/config --nodename xxx --direction upload --ak xxx --sk xxx --region beijing --endpoint ks3-cn-beijing.ksyun.com --bucket multimodal --localfile /root/tmp.log --remotefile s3://logs/tmp.log --runtimeimage shaowenchen/ops-cli Download from Object Storage to Cluster /usr/local/bin/opscli file -i ~/.kube/config --nodename xxx --direction download --ak xxx --sk xxx --region beijing --endpoint ks3-cn-beijing.ksyun.com --bucket multimodal --localfile /root/tmp2.log --remotefile s3://logs/tmp.log --runtimeimage shaowenchen/ops-cli 5. Cluster - Copy Image File to Local To copy an image file from the cluster to the local machine: /usr/local/bin/opscli file -i ~/.kube/config --nodename xxx --direction download --localfile /root/opscli-copy --remotefile shaowenchen/ops-cli:latest:///usr/local/bin/opscli This command helps copy the executable or file from a container/image inside the cluster to the local machine. "},"opscli-case.html":{"url":"opscli-case.html","title":"Case","keywords":"","body":"opscli Usage Examples Test Disk IO Performance on a Specific Node in kubectl Pod Install opscli for alpine sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories apk add curl curl -sfL https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh |VERSION=latest sh - Install fio on the node opscli shell --content \"apt-get install fio -y\" --nodename node1 Test Disk IO performance on the node opscli task -f ~/.ops/tasks/get-diskio-byfio.yaml --size 1g --filename=/tmp/testfile --nodename node1 Where size is the test file size, filename is the test file path, and nodename is the test node name. (1/8) Rand_Read_Testing read: IOPS=105k, BW=410MiB/s (430MB/s)(1024MiB/2498msec) -> 4k Random Read 410 MiB/s (2/8) Rand_Write_Testing write: IOPS=55.9k, BW=218MiB/s (229MB/s)(1024MiB/4688msec) -> 4k Random Write 218 MiB/s (3/8) Sequ_Read_Testing read: IOPS=51.8k, BW=6481MiB/s (6796MB/s)(1024MiB/158msec) -> 128k Sequential Read 6481 MiB/s (4/8) Sequ_Write_Testing write: IOPS=30.7k, BW=3835MiB/s (4022MB/s)(1024MiB/267msec) -> 128k Sequential Write 3835 MiB/s (5/8) Rand_Read_IOPS_Testing read: IOPS=80.4k, BW=314MiB/s (329MB/s)(1024MiB/3261msec) -> 4k Read IOPS 80.4k (6/8) Rand_Write_IOPS_Testing write: IOPS=83.4k, BW=326MiB/s (342MB/s)(1024MiB/3143msec) -> 4k Write IOPS 83.4k (7/8) Rand_Read_Latency_Testing lat (usec): min=34, max=457722, avg=57.78, stdev=1630.32 -> 4k Read Latency 57.78 us (8/8) Rand_Write_Latency_Testing lat (usec): min=35, max=664838, avg=385.12, stdev=5335.64 -> 4k Write Latency 385.12 us Configure Inspection for GPU Hosts in the Cluster Install Opscli on all master nodes opscli task -f ~/.ops/tasks/install-opscli.yaml -i master-ips.txt Create an SSH key to access the hosts from a machine that can SSH into all nodes kubectl -n ops-system create secret generic host-secret --from-file=privatekey=/root/.ssh/id_rsa Add all task templates kubectl apply -f ~/.ops/tasks/ Automatically discover hosts kubectl apply -f - Automatically label hosts kubectl apply -f - GPU Card Drop Inspection kubectl apply -f - GPU ECC Inspection kubectl apply -f - GPU Fabric Inspection kubectl apply -f - GPU Zombie Inspection kubectl apply -f - Disk Cleanup Scheduling kubectl apply -f - "},"opsserver.html":{"url":"opsserver.html","title":"Opsserver","keywords":"","body":"ops-server Overview ops-server is an HTTP service that provides RESTful APIs for interacting with various operations tasks. It can be used for: Remote Command Execution: Execute commands remotely in bulk via HTTP API. File Distribution: Distribute files to multiple hosts via HTTP API. Ops Controller CRD Resource Creation: Create Ops Controller resources (such as Host, Cluster, Task) via HTTP API. Authentication By default, the password for accessing the server is ops. You can customize this password by setting the SERVER_TOKEN environment variable for the server. Object Management ops-server allows you to manage and view resources like Cluster, Host, and Task, as shown in the following illustrations: Clusters Hosts Tasks Task Runs "},"opscontroller.html":{"url":"opscontroller.html","title":"Opscontroller","keywords":"","body":"Ops-controller-manager Overview ops-controller-manager is a Kubernetes Operator that provides three core objects: Host, Cluster, and Task. Objects in ops-controller-manager Host: Describes a host machine, including hostname, IP address, SSH username, password, private key, etc. Cluster: Describes a cluster, including details such as cluster name, number of hosts, number of pods, load, CPU, memory, etc. Task: Describes a task, which can be one-time or scheduled (cron) tasks. Installation Install Helm If you haven't installed Helm yet, you can install it using the following command: curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash Add Helm Repository Add the ops Helm repository to your Helm configuration: helm repo add ops https://www.chenshaowen.com/ops/charts helm repo update Install ops-controller-manager To install the ops-controller-manager using Helm: helm install myops ops/ops --version 2.0.0 --namespace ops-system --create-namespace This command installs the ops-controller-manager in the ops-system namespace and creates the namespace if it doesn't exist. Verify Installation After the installation, check the status of the pods to ensure everything is running: kubectl get pods -n ops-system Uninstall ops-controller-manager To uninstall the ops-controller-manager: helm -n ops-system uninstall myops Namespace Configuration By default, ops-controller-manager will only process CRD resources within the ops-system namespace. To change this behavior, you can modify the ACTIVE_NAMESPACE environment variable in the configuration. If left empty, it will process resources from all namespaces. Make sure to update the namespace if you need ops-controller-manager to work with resources from a different namespace. "},"opscontroller-host.html":{"url":"opscontroller-host.html","title":"Host","keywords":"","body":"Ops-controller-manager Host Object The Host object in the Ops Controller is used to define the configuration for individual hosts that will be managed by Ops. You can create and manage Host objects using opscli commands or YAML files. Create Host Using opscli Command To create a Host object directly using the create sub-command: /usr/local/bin/opscli create host --name dev1 -i 1.1.1.1 --port 2222 --username root --password xxx --namespace ops-system This command creates a host object with the following details: name: The name of the host (e.g., dev1). address: The IP address of the host (e.g., 1.1.1.1). port: The SSH port for accessing the host (e.g., 2222). username: The SSH username (e.g., root). password: The password for SSH access. namespace: The Kubernetes namespace to which the host object belongs (e.g., ops-system). Create Host Using YAML File Alternatively, you can define the Host object in a YAML file and apply it using kubectl. Here is an example YAML definition: apiVersion: crd.chenshaowen.com/v1 kind: Host metadata: name: dev1 namespace: ops-system spec: address: 1.1.1.1 port: 2222 privatekey: base64 encoded private key username: root privatekeypath: ~/.ssh/id_rsa timeoutseconds: 10 In this YAML file: address: The IP address of the host. port: The SSH port number for the host. privatekey: The base64-encoded private SSH key for authentication. privatekeypath: The path to the private SSH key file. username: The username for SSH access. timeoutseconds: The timeout value (in seconds) for SSH connections. You can apply this file using the following command: kubectl apply -f host.yaml View Host Object Status To view the status of the Host object, use the following command: kubectl get hosts dev1 -n ops-system This will return information about the Host object, including: NAME: The name of the host. HOSTNAME: The hostname of the machine (e.g., node1). ADDRESS: The IP address of the host. DISTRIBUTION: The OS distribution (e.g., centos). ARCH: The architecture of the host (e.g., x86_64). CPU: The number of CPUs on the host. MEM: The amount of memory on the host (e.g., 7.8G). DISK: The amount of disk space on the host (e.g., 52G). HEARTTIME: The last time the host was checked. HEARTSTATUS: The status of the heartbeats (e.g., successed). Example output: NAME HOSTNAME ADDRESS DISTRIBUTION ARCH CPU MEM DISK HEARTTIME HEARTSTATUS dev1 node1 1.1.1.1 centos x86_64 4 7.8G 52G 54s successed "},"opscontroller-cluster.html":{"url":"opscontroller-cluster.html","title":"Cluster","keywords":"","body":"Ops-controller-manager Cluster Object The Cluster object in the Ops Controller can be created and managed using opscli commands or YAML files. Create Cluster Using opscli Command To create a cluster directly using the create sub-command: /usr/local/bin/opscli create cluster -i ~/.kube/config --name dev1 --namespace ops-system This command creates a cluster named dev1 in the ops-system namespace, using the default kubeconfig file located at ~/.kube/config. Create Cluster Using YAML File Alternatively, you can define the Cluster object in a YAML file and apply it using kubectl. Here is an example YAML definition: apiVersion: crd.chenshaowen.com/v1 kind: Cluster metadata: name: dev1 namespace: ops-system spec: config: base64 encoded kubeadm config server: https://1.1.1.1:6443 In this YAML file: config: Should contain the base64-encoded kubeadm configuration. server: The address of the Kubernetes API server. You can apply this file using the following command: kubectl apply -f cluster.yaml View Cluster Object Status To view the status of the Cluster object, use the following command: kubectl get cluster dev1 -n ops-system This will return information about the Cluster object, including: NAME: The name of the cluster. SERVER: The address of the Kubernetes API server. VERSION: The Kubernetes version. NODE: The number of nodes. RUNNING: The number of running nodes. TOTALPOD: The total number of pods in the cluster. CERTDAYS: The remaining days of the cluster's certificate validity. STATUS: The current status of the cluster (e.g., successed). Example output: NAME SERVER VERSION NODE RUNNING TOTALPOD CERTDAYS STATUS dev1 https://1.1.1.1:6443 v1.21.0 1 15 16 114 successed "},"opscontroller-task.html":{"url":"opscontroller-task.html","title":"Task","keywords":"","body":"Ops-controller-manager Task Object The Task object in the Ops Controller defines the operations or tasks that need to be executed, either on specific hosts or within the Ops Controller pod. You can create and manage Task objects using opscli commands or YAML files. Create Task Using opscli Command To create a Task object directly using the create sub-command: /usr/local/bin/opscli create task --name t1 --typeref host --nameref dev1 --filepath ./task/get-osstaus.yaml name: The name of the task (e.g., t1). typeref: Specifies the type of resource the task will execute on (e.g., host). nameref: Specifies the name of the host (e.g., dev1). filepath: The path to the YAML file containing the task definition. Create Task Using YAML File Alternatively, you can define the Task object in a YAML file and apply it using kubectl. Below is an example YAML file for a scheduled task that checks the HTTP status of a URL every minute and sends a notification if the status code is not 200. apiVersion: crd.chenshaowen.com/v1 kind: Task metadata: name: alert-http-status-dockermirror namespace: ops-system spec: desc: alert crontab: \"*/1 * * * *\" variables: url: default: http://1.1.1.1:5000/ expect: default: \"200\" message: default: ${url} http status is not ${expect} steps: - name: get status content: curl -I -m 10 -o /dev/null -s -w %{http_code} ${url} - name: notifaction when: ${result} != ${expect} content: | curl -X POST 'https://365.kdocs.cn/woa/api/v1/webhook/send?key=xxx' -H 'content-type: application/json' -d '{ \"msgtype\": \"text\", \"text\": { \"content\": \"${message}\" } }' In this YAML: desc: A description of the task (e.g., alert). crontab: The cron schedule (e.g., */1 * * * * means every minute). variables: Defines task-specific variables, like url, expect, and message. steps: Defines the individual operations or commands that should be executed: get status: Executes a curl command to get the HTTP status code. notifaction: Sends a notification if the HTTP status code does not match the expected value. You can apply this file using: kubectl apply -f task.yaml View Task Object Status To view the status of a specific Task object, use the following command: kubectl get task t1 -n ops-system To list all tasks in the ops-system namespace: kubectl get task -n ops-system Example output: NAME CRONTAB TYPEREF NAMEREF NODENAME ALL STARTTIME RUNSTATUS alert-http-status-dockermirror */1 * * * * host dev1 node1 true 2024-11-09 successed In the output: NAME: The name of the task. CRONTAB: The cron schedule for the task. TYPEREF: The type of the resource the task will run on (e.g., host). NAMEREF: The reference name of the target resource (e.g., dev1). NODENAME: The name of the node where the task is running. ALL: Indicates whether the task runs on all nodes. STARTTIME: The time the task was started. RUNSTATUS: The current status of the task (e.g., successed). "},"nats.html":{"url":"nats.html","title":"Nats","keywords":"","body":"NATS Purpose Ops uses the NATS component to export relevant events, primarily of two types: The status of CRDs, including the status of hosts, clusters, TaskRun, and PipelineRun. Status information reported by scheduled inspections from alerts. Below is a guide for installing and configuring the NATS component. This setup follows a model with one primary cluster and multiple edge clusters. The edge clusters forward events to the primary cluster for unified processing. Adding the Helm Repo Add the repository: helm repo add nats https://nats-io.github.io/k8s/helm/charts/ helm repo update View configurable fields: helm show values nats/nats Deploying the Primary Cluster Set basic NATS credentials: export adminpassword=mypassword export apppassword=mypassword Generate nats-values.yaml: cat nats-values.yaml config: jetstream: enabled: true fileStore: enabled: false dir: /data memoryStore: enabled: true maxSize: 1Gi pvc: enabled: false storageClassName: my-sc cluster: enabled: true leafnodes: enabled: true merge: accounts: SYS: users: - user: admin password: ${adminpassword} APP: users: - user: app password: ${apppassword} jetstream: true system_account: SYS container: image: repository: nats tag: 2.10.20-alpine natsBox: container: image: repository: nats-box tag: 0.14.5 reloader: enabled: true image: repository: natsio/nats-server-config-reloader tag: 0.15.1 EOF Data is persisted in memory. To store it on disk, you need to configure fileStore. Install NATS: helm -n ops-system install nats nats/nats --version 1.2.4 -f nats-values.yaml Uninstall NATS: helm -n ops-system uninstall nats Expose the NATS service ports: kubectl patch svc nats -p '{\"spec\":{\"type\":\"NodePort\",\"ports\":[{\"port\":4222,\"nodePort\":32223,\"targetPort\":\"nats\"},{\"port\":7422,\"nodePort\":32222,\"targetPort\":\"leafnodes\"}]}}' -n ops-system Check the workload: kubectl -n ops-system get pod,svc | grep nats pod/nats-0 2/2 Running 0 15h pod/nats-1 2/2 Running 0 15h pod/nats-2 2/2 Running 0 15h pod/nats-box-6bb86df889-xcr6x 1/1 Running 0 15h service/nats NodePort 10.100.109.24 4222:32223/TCP,7422:32222/TCP 15h service/nats-headless ClusterIP None 4222/TCP,7422/TCP,6222/TCP,8222/TCP 15h Deploying Edge Clusters Add the repository: helm repo add nats https://nats-io.github.io/k8s/helm/charts/ helm repo update Set the primary cluster's NATS information: export natsendpoint=x.x.x.x:32222 Set the NATS server name: export natsservername=need-to-be-unique Generate nats-values.yaml: Note that the server_name must be unique for each cluster; otherwise, duplicate connection issues will arise. cat nats-values.yaml config: leafnodes: enabled: true merge: remotes: - urls: - nats://admin:${adminpassword}@${natsendpoint} account: SYS - urls: - nats://app:${apppassword}@${natsendpoint} account: APP merge: server_name: ${natsservername} accounts: SYS: users: - user: admin password: ${adminpassword} APP: users: - user: app password: ${apppassword} jetstream: true system_account: SYS container: image: repository: nats tag: 2.10.20-alpine natsBox: container: image: repository: nats-box tag: 0.14.5 reloader: enabled: true image: repository: natsio/nats-server-config-reloader tag: 0.15.1 EOF Install NATS: helm install nats nats/nats --version 1.2.4 -f nats-values.yaml -n ops-system Common NATS Commands Test NATS: kubectl -n ops-system exec -it deployment/nats-box -- sh Subscribe to messages: nats --user=app --password=${apppassword} sub \"ops.>\" Publish messages: nats --user=app --password=${apppassword} pub ops.test \"mymessage mycontent\" Create a stream to persist messages: nats --user=app --password=${apppassword} stream add ops --subjects \"ops.>\" --ack --max-msgs=-1 --max-bytes=-1 --max-age=168h --storage file --retention limits --max-msg-size=-1 --discard=old --replicas 1 --dupe-window=2m For production environments, it is recommended to use file storage and set replicas to 3. View stream events: nats --user=app --password=${apppassword} stream view ops View stream configuration: nats --user=app --password=${apppassword} stream info ops View cluster information: nats --user=admin --password=${adminpassword} server report jetstream This command displays information about the primary cluster, edge clusters, and their connections. View the subjects of a stream: nats --user=app --password=${apppassword} stream subjects ops Perform a benchmark: nats --user=app --password=${apppassword} bench benchsubject --pub 1 --sub 10 References JetStream Configuration LeafNode Configuration Gateway Configuration "}}
{"./":{"url":"./","title":"Introduction","keywords":"","body":"Ops Overview Ops is an operations tool designed to provide a simple and efficient platform for system administrators to complete maintenance tasks quickly. It aims to streamline operations, with a focus on automation and task management across multiple systems and clusters. Production Use Cases CICD Clusters: Building 2k+ daily for CICD clusters. Global Clusters: Managing 40+ clusters overseas. AI Computing Clusters: Managing 20+ AI computing clusters. Architecture Support: Supporting both ARM and X86 architectures. Design Overview The core components of Ops are built around objects that represent hosts, clusters, and tasks. The design allows for efficient orchestration of operations in a Kubernetes-native environment. Key Objects: Host: Represents machines (cloud-based or bare-metal) that can be accessed via SSH. Cluster: Represents Kubernetes clusters that can be accessed via kubectl. Task: Represents a combination of multiple files and shell commands. Pipeline: Represents a sequence of tasks executed in a specific order. Core Operations: File: Uploading and distributing files to hosts and clusters. Shell: Executing shell scripts on remote hosts or clusters. Components ops-cli: A command-line tool that assists system administrators with automation tasks. ops-server: An HTTP service that provides RESTful APIs and a Dashboard interface for managing and monitoring tasks and resources. ops-controller: A Kubernetes Operator that manages hosts, clusters, tasks, pipelines, and other resources in the Kubernetes environment. Multi-Cluster Support In practice, it is recommended to: Host Creation: Create hosts based on the current clusterâ€™s machines. Cluster Creation: Multiple clusters can be added, each cluster will be treated as a managed cluster. Task and Pipeline objects are automatically synchronized across all clusters under management, removing the need for manual triggering. When deploying a pipeline, a PipelineRun object is created, which can span multiple clusters. Unlike TaskRuns, PipelineRuns can cross clusters. The ops-controller watches the PipelineRun object, and based on the cluster field, it dispatches the pipeline to the corresponding cluster's controller, which executes the tasks and updates the status of the PipelineRun. Event-Driven Architecture Ops adopts an event-driven approach to manage operations: Heartbeat Events: These events are triggered periodically to check the health of hosts and clusters. Task Execution Events: Events that are triggered when a TaskRun or PipelineRun task is executed. Inspection Events: Triggered during scheduled tasks or inspections to monitor the system. Webhook Events: Custom events like alerts and notifications for maintenance activities. Event Aggregation in Multi-Cluster Setup: In a multi-cluster environment, each cluster is recommended to install a Nats component to collect events from edge clusters. Events are then aggregated into one or more clusters to centralize the monitoring and management of operational tasks. System Architecture The overall architecture is built to handle large-scale, multi-cluster environments, ensuring that tasks and pipelines can be seamlessly distributed and monitored across multiple regions and systems. The ops-server acts as a central hub for managing tasks and pipelines, while the ops-controller manages resources in the Kubernetes clusters. This system is designed to provide comprehensive and automated operations management, facilitating the handling of complex tasks across diverse infrastructures. "},"opscli.html":{"url":"opscli.html","title":"Opscli","keywords":"","body":"opscli Overview opscli is a command-line interface (CLI) designed to facilitate batch remote execution of commands, file distribution, and management of Ops Controller CRD resources. It primarily deals with three types of CRDs: Host, Cluster, and Task. Main Features of opscli Batch Remote Command ExecutionAllows you to run commands across multiple remote hosts or nodes in a cluster. Batch File DistributionEnables bulk file distribution to remote systems. Ops Controller CRD ResourcesFacilitates the creation and management of Host, Cluster, and Task resources in the Ops Controller. Supported Operating Systems Linux macOS "},"opscli-install.html":{"url":"opscli-install.html","title":"Install","keywords":"","body":"opscli Installation Guide 1. Single Machine Installation For Domestic Users (China) PROXY=https://ghfast.top/ curl -sfL $PROXY/https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh | VERSION=latest PROXY=$PROXY sh - For International Users (Outside China) curl -sfL https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh | VERSION=latest sh - 2. Batch Installation To install opscli on multiple hosts, list the IP addresses of all hosts in a hosts.txt file, and use the opscli shell command to execute the installation. The default credential is the current user's ~/.ssh/id_rsa. For Domestic Users (China) /usr/local/bin/opscli shell --content \"curl -sfL https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh | VERSION=latest sh -\" -i hosts.txt For International Users (Outside China) /usr/local/bin/opscli shell --content \"curl -sfL https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh | VERSION=latest sh -\" -i hosts.txt 3. Version Upgrade For Single Machine sudo /usr/local/bin/opscli upgrade For Batch Upgrade /usr/local/bin/opscli shell --content \"sudo /usr/local/bin/opscli upgrade\" -i hosts.txt 4. Auto-completion Setup For bash echo 'source >~/.bashrc For zsh echo 'source >~/.zshrc 5. Configuration opscli supports configuration management through the config command, allowing you to set and manage configuration values that are used across all CLI commands. Configuration File Location Configuration is stored in ~/.ops/opscli/config (YAML format). Supported Configuration Keys proxy: Proxy URL for network requests (e.g., https://ghfast.top/) runtimeimage: Default runtime image for Kubernetes tasks (e.g., ubuntu:22.04) Configuration Commands Set configuration: opscli config set opscli config set proxy https://ghfast.top/ opscli config set runtimeimage ubuntu:22.04 Get configuration: opscli config get opscli config get proxy List all configurations: opscli config list opscli config list # Output: # proxy = https://ghfast.top/ # runtimeimage = (not set) Unset configuration: opscli config unset opscli config unset proxy Configuration Priority Configuration values follow a priority order (highest to lowest): CLI Arguments (highest priority) Command-line flags like --proxy or --runtimeimage Example: opscli task --filepath task.yaml --proxy https://cli-proxy.com Environment Variables PROXY: Proxy URL DEFAULT_RUNTIME_IMAGE: Default runtime image Example: export PROXY=https://env-proxy.com Configuration File (~/.ops/opscli/config) Values set via opscli config set Example: opscli config set proxy https://config-proxy.com Default Values (lowest priority) Built-in defaults Proxy: https://ghproxy.chenshaowen.com/ Runtime Image: ubuntu:22.04 Usage Examples # Example 1: Using configuration file opscli config set proxy https://ghfast.top/ opscli upgrade --manifests # Automatically uses proxy from config # Example 2: Override with environment variable export PROXY=https://env-proxy.com opscli upgrade --manifests # Uses environment variable # Example 3: Override with CLI argument (highest priority) opscli upgrade --proxy https://cli-proxy.com # Uses CLI argument 6. More Information To see additional usage options: /usr/local/bin/opscli --help "},"opscli-shell.html":{"url":"opscli-shell.html","title":"Shell","keywords":"","body":"opscli Shell Command Guide 1. Specify Target Hosts Single Host Use the -i flag to specify a single host IP. -i 1.1.1.1 You can also specify a username and password with the --username and --password flags: --username --password Batch Hosts To specify multiple hosts, you can use a file or comma-separated IP addresses. From a File (hosts.txt) -i hosts.txt Example content of hosts.txt: 1.1.1.1 2.2.2.2 Comma-separated IPs -i 1.1.1.1,2.2.2.2 All Nodes in a Cluster -i ~/.kube/config --nodename all By default, -i points to ~/.kube/config. Specific Node in a Cluster -i ~/.kube/config --nodename node1 Where node1 is the node name. 2. View Cluster Images For Single Machine /usr/local/bin/opscli task -f ~/.ops/tasks/list-podimage.yaml --namespace all 3. Cluster Bulk Operations All Nodes To run a command on all nodes: opscli shell --content \"uname -a\" --nodename all Specific Node To run a command on a specific node: opscli shell --content \"uname -a\" --nodename node1 Specify kubeconfig To specify a custom kubeconfig, use the -i flag: opscli shell -i ~/Documents/opscli/prod --content \"uname -a\" --nodename node1 4. Mount Host Paths to Container The --mount flag allows you to mount host paths into the container. This is useful for accessing host files, directories, or socket files (like docker.sock) from within the container. Single Mount opscli shell --content \"ls /data\" --mount /opt/data:/data Multiple Mounts You can specify multiple mounts by using --mount multiple times: opscli shell --content \"ls /data /logs\" \\ --mount /opt/data:/data \\ --mount /opt/logs:/logs Mount Docker Socket To mount the Docker socket and use Docker commands inside the container: opscli shell --content \"docker ps\" \\ --mount /var/run/docker.sock:/var/run/docker.sock Mount Format The mount format is: hostPath:mountPath hostPath: absolute path on the host (required) mountPath: absolute path in the container (required) Note: If you need to access the host root filesystem, you can mount it explicitly: opscli shell --content \"ls /host\" --mount /:/host "},"opscli-task.html":{"url":"opscli-task.html","title":"Task","keywords":"","body":"opscli Task Command Guide 1. Specify Target Hosts Single Host -i 1.1.1.1 You can specify the username and password for the host with --username and --password. Batch Hosts To specify multiple hosts via a file: -i hosts.txt Example content of hosts.txt: 1.1.1.1 2.2.2.2 All Nodes in a Cluster -i ~/.kube/config --nodename all Specific Node in a Cluster -i ~/.kube/config --nodename node1 Where node1 is the node name. 2. Update /etc/hosts On a Single Host To remotely update the /etc/hosts file on host 1.1.1.1: /usr/local/bin/opscli task -f ~/.ops/tasks/set-hosts.yaml --ip 1.2.3.4 --domain test.com --i 1.1.1.1 --port 2222 --username root To clear the /etc/hosts entry, add --clear: /usr/local/bin/opscli task -f ~/.ops/tasks/set-hosts.yaml --ip 1.2.3.4 --domain test.com --i 1.1.1.1 --clear On All Nodes in a Cluster /usr/local/bin/opscli task -f ~/.ops/tasks/set-hosts.yaml --ip 1.2.3.4 --domain test.com --i ~/.kube/config --nodename all On a Specific Node in a Cluster /usr/local/bin/opscli task -f ~/.ops/tasks/set-hosts.yaml --ip 1.2.3.4 --domain test.com --i ~/.kube/config --nodename node1 3. Install Applications Install Istio /usr/local/bin/opscli task -f ~/.ops/tasks/app-istio.yaml --version 1.13.7 --kubeconfig /etc/kubernetes/admin.conf The default version is 1.13.7 and the default kubeconfig is /etc/kubernetes/admin.conf. Uninstall Istio /usr/local/bin/opscli task -f ~/.ops/tasks/app-istio.yaml --version 1.13.7 --kubeconfig /etc/kubernetes/admin.conf --action delete 4. Upload Files Upload to Server /usr/local/bin/opscli task -f tasks/file-upload.yaml --fileapi https://gh-uploadapi.chenshaowen.com/api/v1/files --localfile dockerfile Upload to S3 /usr/local/bin/opscli task -f tasks/file-upload.yaml --ak xxx --sk xxx --region beijing --endpoint ks3-cn-beijing.ksyun.com --bucket xxx --localfile dockerfile --remotefile s3://dockerfile 5. Download Files From Server /usr/local/bin/opscli task -f tasks/file-download.yaml --fileapi https://gh-uploadapi.chenshaowen.com/api/v1/files --aeskey --remotefile --localfile dockerfile1 From S3 /usr/local/bin/opscli task -f tasks/file-download.yaml --ak xxx --sk xxx --region beijing --endpoint ks3-cn-beijing.ksyun.com --bucket xxx --localfile dockerfile2 --remotefile s3://dockerfile "},"opscli-file.html":{"url":"opscli-file.html","title":"File","keywords":"","body":"opscli file Command Usage The opscli file command is used for transferring files between the local host, object storage, API servers, and clusters. Below are the details for various use cases. 1. Host - Local and Object Storage File Transfer Set AK/SK (Access Key / Secret Key) export ak= export sk= Upload a Local File to Object Storage To upload a file ./tmp.log to the object storage at s3://logs/tmp.log: /usr/local/bin/opscli file --direction upload --localfile ./tmp.log --remotefile s3://logs/tmp.log --bucket obs-test Here: --bucket is the S3 bucket name. --region is the S3 bucket's region. --endpoint is the S3 bucket's endpoint. --direction specifies the upload direction. --localfile is the local file to be uploaded. --remotefile is the destination file in object storage. Download a File from S3 to Local To download s3://logs/tmp.log to the local file ./tmp1.log: /usr/local/bin/opscli file --direction download --localfile ./tmp1.log --remotefile s3://logs/tmp.log --bucket obs-test Unset AK/SK To clear the AK/SK environment variables: unset ak unset sk 2. Host - Local and API Server File Transfer This option provides encryption/decryption for file transfers with the API server. Upload to API Server /usr/local/bin/opscli file --direction upload --fileapi https://gh-uploadapi.chenshaowen.com/api/v1/files --localfile ./tmp.log If aeskey is \"\", a random encryption key is generated automatically. If not set, the file is uploaded without encryption. Download from API Server /usr/local/bin/opscli file --fileapi https://gh-uploadapi.chenshaowen.com/api/v1/files --aeskey xxx --direction download --remotefile https://download_url_link.com.aes 3. Cluster - Local and API Server File Transfer Upload to Cluster's API Server /usr/local/bin/opscli file -i ~/.kube/config --nodename node1 --direction upload --fileapi https://gh-uploadapi.chenshaowen.com/api/v1/files --aeskey \"\" --localfile /root/tmp.log --runtimeimage shaowenchen/ops-cli Download from Cluster's API Server /usr/local/bin/opscli file -i ~/.kube/config --nodename xxx --direction download --fileapi https://gh-uploadapi.chenshaowen.com/api/v1/files --aeskey xxx --localfile /root/tmp1.log --remotefile https://gh-uploadapi.chenshaowen.com/uploadbases/cdn0/raw/1721621949-tmp.log.aes --runtimeimage shaowenchen/ops-cli 4. Cluster - Local and Object Storage File Transfer Upload to Object Storage from Cluster /usr/local/bin/opscli file -i ~/.kube/config --nodename xxx --direction upload --ak xxx --sk xxx --region beijing --endpoint ks3-cn-beijing.ksyun.com --bucket multimodal --localfile /root/tmp.log --remotefile s3://logs/tmp.log --runtimeimage shaowenchen/ops-cli Download from Object Storage to Cluster /usr/local/bin/opscli file -i ~/.kube/config --nodename xxx --direction download --ak xxx --sk xxx --region beijing --endpoint ks3-cn-beijing.ksyun.com --bucket multimodal --localfile /root/tmp2.log --remotefile s3://logs/tmp.log --runtimeimage shaowenchen/ops-cli 5. Cluster - Copy Image File to Local To copy an image file from the cluster to the local machine: /usr/local/bin/opscli file -i ~/.kube/config --nodename xxx --direction download --localfile /root/opscli-copy --remotefile shaowenchen/ops-cli:latest:///usr/local/bin/opscli This command helps copy the executable or file from a container/image inside the cluster to the local machine. 6. Mount Host Paths to Container The --mount flag allows you to mount host paths into the container when transferring files. This is useful for accessing host files or directories from within the container. Single Mount opscli file -i ~/.kube/config --nodename node1 \\ --direction upload \\ --localfile /root/tmp.log \\ --remotefile s3://logs/tmp.log \\ --mount /opt/data:/data Multiple Mounts You can specify multiple mounts by using --mount multiple times: opscli file -i ~/.kube/config --nodename node1 \\ --direction upload \\ --localfile /root/tmp.log \\ --remotefile s3://logs/tmp.log \\ --mount /opt/data:/data \\ --mount /opt/logs:/logs Mount Format The mount format is: hostPath:mountPath hostPath: absolute path on the host (required) mountPath: absolute path in the container (required) Note: If you need to access the host root filesystem for file operations, you can mount it explicitly: opscli file --mount /:/host ... "},"opscli-case.html":{"url":"opscli-case.html","title":"Case","keywords":"","body":"opscli Usage Examples Test Disk IO Performance on a Specific Node in kubectl Pod Install opscli for alpine sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories apk add curl curl -sfL https://ghproxy.chenshaowen.com/https://raw.githubusercontent.com/shaowenchen/ops/main/getcli.sh |VERSION=latest sh - Install fio on the node opscli shell --content \"apt-get install fio -y\" --nodename node1 Test Disk IO performance on the node opscli task -f ~/.ops/tasks/get-diskio-byfio.yaml --size 1g --filename=/tmp/testfile --nodename node1 Where size is the test file size, filename is the test file path, and nodename is the test node name. (1/8) Rand_Read_Testing read: IOPS=105k, BW=410MiB/s (430MB/s)(1024MiB/2498msec) -> 4k Random Read 410 MiB/s (2/8) Rand_Write_Testing write: IOPS=55.9k, BW=218MiB/s (229MB/s)(1024MiB/4688msec) -> 4k Random Write 218 MiB/s (3/8) Sequ_Read_Testing read: IOPS=51.8k, BW=6481MiB/s (6796MB/s)(1024MiB/158msec) -> 128k Sequential Read 6481 MiB/s (4/8) Sequ_Write_Testing write: IOPS=30.7k, BW=3835MiB/s (4022MB/s)(1024MiB/267msec) -> 128k Sequential Write 3835 MiB/s (5/8) Rand_Read_IOPS_Testing read: IOPS=80.4k, BW=314MiB/s (329MB/s)(1024MiB/3261msec) -> 4k Read IOPS 80.4k (6/8) Rand_Write_IOPS_Testing write: IOPS=83.4k, BW=326MiB/s (342MB/s)(1024MiB/3143msec) -> 4k Write IOPS 83.4k (7/8) Rand_Read_Latency_Testing lat (usec): min=34, max=457722, avg=57.78, stdev=1630.32 -> 4k Read Latency 57.78 us (8/8) Rand_Write_Latency_Testing lat (usec): min=35, max=664838, avg=385.12, stdev=5335.64 -> 4k Write Latency 385.12 us Configure Inspection for GPU Hosts in the Cluster Install Opscli on all master nodes opscli task -f ~/.ops/tasks/install-opscli.yaml -i master-ips.txt Create an SSH key to access the hosts from a machine that can SSH into all nodes kubectl -n ops-system create secret generic host-secret --from-file=privatekey=/root/.ssh/id_rsa Add all task templates kubectl apply -f ~/.ops/tasks/ Automatically discover hosts kubectl apply -f - Automatically label hosts kubectl apply -f - GPU Card Drop Inspection kubectl apply -f - GPU ECC Inspection kubectl apply -f - GPU Fabric Inspection kubectl apply -f - GPU Zombie Inspection kubectl apply -f - Disk Cleanup Scheduling kubectl apply -f - "},"opsserver.html":{"url":"opsserver.html","title":"Opsserver","keywords":"","body":"ops-server Overview ops-server is an HTTP service that provides RESTful APIs for interacting with various operations tasks. It can be used for: Remote Command Execution: Execute commands remotely in bulk via HTTP API. File Distribution: Distribute files to multiple hosts via HTTP API. Ops Controller CRD Resource Creation: Create Ops Controller resources (such as Host, Cluster, Task) via HTTP API. Authentication By default, the password for accessing the server is ops. You can customize this password by setting the SERVER_TOKEN environment variable for the server. Object Management ops-server allows you to manage and view resources like Cluster, Host, and Task, as shown in the following illustrations: Clusters Hosts Tasks Task Runs "},"opscontroller.html":{"url":"opscontroller.html","title":"Opscontroller","keywords":"","body":"Ops-controller-manager Overview ops-controller-manager is a Kubernetes Operator that provides three core objects: Host, Cluster, and Task. Objects in ops-controller-manager Host: Describes a host machine, including hostname, IP address, SSH username, password, private key, etc. Cluster: Describes a cluster, including details such as cluster name, number of hosts, number of pods, load, CPU, memory, etc. Task: Describes a task, which can be one-time or scheduled (cron) tasks. Installation Prerequisites Kubernetes 1.19+ Helm 3.0+ NATS server (optional but recommended for event streaming) Install Helm If you haven't installed Helm yet, you can install it using the following command: curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash Add Helm Repository Add the ops Helm repository to your Helm configuration: helm repo add ops https://www.chenshaowen.com/ops/charts helm repo update Install ops-controller-manager Basic Installation: To install the ops-controller-manager using Helm with default values: helm install myops ops/ops --version 1.2.0 --namespace ops-system --create-namespace Installation with Custom Values: You can customize the installation using --set flags: helm install myops ops/ops --version 1.2.0 \\ --namespace ops-system \\ --create-namespace \\ --set controller.image.repository=\"shaowenchen/ops-controller-manager\" \\ --set controller.image.pullPolicy=\"Always\" \\ --set controller.image.tag=\"latest\" \\ --set controller.env.activeNamespace=\"ops-system\" \\ --set controller.env.defaultRuntimeImage=\"ubuntu:22.04\" \\ --set controller.replicaCount=2 \\ --set server.image.repository=\"shaowenchen/ops-server\" \\ --set server.image.pullPolicy=\"Always\" \\ --set server.image.tag=\"latest\" \\ --set server.replicaCount=2 \\ --set server.autoscaling.minReplicas=2 \\ --set server.autoscaling.maxReplicas=4 \\ --set event.cluster=\"mycluster\" \\ --set event.endpoint=\"http://app:password@nats-headless.ops-system.svc:4222\" Installation with Values File: Create a custom values file for more complex configurations: # my-values.yaml event: cluster: \"mycluster\" endpoint: \"http://app:password@nats-headless.ops-system.svc:4222\" controller: replicaCount: 2 image: repository: shaowenchen/ops-controller-manager pullPolicy: Always tag: \"latest\" env: activeNamespace: \"ops-system\" defaultRuntimeImage: \"ubuntu:22.04\" server: replicaCount: 2 image: repository: shaowenchen/ops-server pullPolicy: Always tag: \"latest\" autoscaling: enabled: true minReplicas: 2 maxReplicas: 4 targetCPUUtilizationPercentage: 80 prometheus: enabled: true Then install with the values file: helm install myops ops/ops --version 1.2.0 \\ --namespace ops-system \\ --create-namespace \\ -f my-values.yaml Verify Installation After the installation, check the status of the pods to ensure everything is running: # Check pods kubectl get pods -n ops-system # Check services kubectl get svc -n ops-system # Check deployments kubectl get deployments -n ops-system Upgrade Installation To upgrade an existing installation: helm upgrade myops ops/ops --version 1.2.0 --namespace ops-system Or with custom values: helm upgrade myops ops/ops --version 1.2.0 \\ --namespace ops-system \\ -f my-values.yaml Uninstall ops-controller-manager To uninstall the ops-controller-manager: helm uninstall myops --namespace ops-system Configuration Namespace Configuration By default, ops-controller-manager will only process CRD resources within the ops-system namespace. To change this behavior, you can modify the controller.env.activeNamespace value in the Helm values. If left empty, it will process resources from all namespaces. Using --set: helm install myops ops/ops --version 1.2.0 \\ --namespace ops-system \\ --create-namespace \\ --set controller.env.activeNamespace=\"\" Using values file: controller: env: activeNamespace: \"\" # Empty means all namespaces Event Configuration Configure NATS event endpoint for both controller and server: controller: env: eventCluster: \"default\" eventEndpoint: \"http://app:password@nats-headless.ops-system.svc:4222\" server: env: eventCluster: \"default\" eventEndpoint: \"http://app:password@nats-headless.ops-system.svc:4222\" Resource Configuration Configure resource limits and requests: resources: limits: cpu: 2000m memory: 4096Mi requests: cpu: 1000m memory: 2048Mi You can also configure separate resources for server: server: resources: limits: cpu: 1000m memory: 2048Mi requests: cpu: 500m memory: 1024Mi Monitoring Configuration Enable Prometheus monitoring (creates ServiceMonitor resources): prometheus: enabled: true Autoscaling Configuration Enable Horizontal Pod Autoscaler: autoscaling: enabled: true minReplicas: 2 maxReplicas: 10 targetCPUUtilizationPercentage: 80 targetMemoryUtilizationPercentage: 80 Troubleshooting Check Pod Logs # Controller logs kubectl logs -n ops-system deployment/myops-ops # Server logs kubectl logs -n ops-system deployment/myops-ops-server Check Metrics Endpoints # Controller metrics kubectl port-forward -n ops-system svc/myops-ops-controller-metrics 8080:8080 curl http://localhost:8080/metrics # Server metrics kubectl port-forward -n ops-system svc/myops-ops-server 8080:80 curl http://localhost:8080/metrics Check ServiceMonitors kubectl get servicemonitor -n ops-system For more detailed configuration options, see the Helm Chart README. "},"opscontroller-host.html":{"url":"opscontroller-host.html","title":"Host","keywords":"","body":"Ops-controller-manager Host Object The Host object in the Ops Controller is used to define the configuration for individual hosts that will be managed by Ops. You can create and manage Host objects using opscli commands or YAML files. Create Host Using opscli Command To create a Host object directly using the create sub-command: /usr/local/bin/opscli create host --name dev1 -i 1.1.1.1 --port 2222 --username root --password xxx --namespace ops-system This command creates a host object with the following details: name: The name of the host (e.g., dev1). address: The IP address of the host (e.g., 1.1.1.1). port: The SSH port for accessing the host (e.g., 2222). username: The SSH username (e.g., root). password: The password for SSH access. namespace: The Kubernetes namespace to which the host object belongs (e.g., ops-system). Create Host Using YAML File Alternatively, you can define the Host object in a YAML file and apply it using kubectl. Here is an example YAML definition: apiVersion: crd.chenshaowen.com/v1 kind: Host metadata: name: dev1 namespace: ops-system spec: address: 1.1.1.1 port: 2222 privatekey: base64 encoded private key username: root privatekeypath: ~/.ssh/id_rsa timeoutseconds: 10 In this YAML file: address: The IP address of the host. port: The SSH port number for the host. privatekey: The base64-encoded private SSH key for authentication. privatekeypath: The path to the private SSH key file. username: The username for SSH access. timeoutseconds: The timeout value (in seconds) for SSH connections. You can apply this file using the following command: kubectl apply -f host.yaml View Host Object Status To view the status of the Host object, use the following command: kubectl get hosts dev1 -n ops-system This will return information about the Host object, including: NAME: The name of the host. HOSTNAME: The hostname of the machine (e.g., node1). ADDRESS: The IP address of the host. DISTRIBUTION: The OS distribution (e.g., centos). ARCH: The architecture of the host (e.g., x86_64). CPU: The number of CPUs on the host. MEM: The amount of memory on the host (e.g., 7.8G). DISK: The amount of disk space on the host (e.g., 52G). HEARTTIME: The last time the host was checked. HEARTSTATUS: The status of the heartbeats (e.g., successed). Example output: NAME HOSTNAME ADDRESS DISTRIBUTION ARCH CPU MEM DISK HEARTTIME HEARTSTATUS dev1 node1 1.1.1.1 centos x86_64 4 7.8G 52G 54s successed "},"opscontroller-cluster.html":{"url":"opscontroller-cluster.html","title":"Cluster","keywords":"","body":"Ops-controller-manager Cluster Object The Cluster object in the Ops Controller can be created and managed using opscli commands or YAML files. Create Cluster Using opscli Command To create a cluster directly using the create sub-command: /usr/local/bin/opscli create cluster -i ~/.kube/config --name dev1 --namespace ops-system This command creates a cluster named dev1 in the ops-system namespace, using the default kubeconfig file located at ~/.kube/config. Create Cluster Using YAML File Alternatively, you can define the Cluster object in a YAML file and apply it using kubectl. Here is an example YAML definition: apiVersion: crd.chenshaowen.com/v1 kind: Cluster metadata: name: dev1 namespace: ops-system spec: config: base64 encoded kubeadm config server: https://1.1.1.1:6443 In this YAML file: config: Should contain the base64-encoded kubeadm configuration. server: The address of the Kubernetes API server. You can apply this file using the following command: kubectl apply -f cluster.yaml View Cluster Object Status To view the status of the Cluster object, use the following command: kubectl get cluster dev1 -n ops-system This will return information about the Cluster object, including: NAME: The name of the cluster. SERVER: The address of the Kubernetes API server. VERSION: The Kubernetes version. NODE: The number of nodes. RUNNING: The number of running nodes. TOTALPOD: The total number of pods in the cluster. CERTDAYS: The remaining days of the cluster's certificate validity. STATUS: The current status of the cluster (e.g., successed). Example output: NAME SERVER VERSION NODE RUNNING TOTALPOD CERTDAYS STATUS dev1 https://1.1.1.1:6443 v1.21.0 1 15 16 114 successed "},"opscontroller-task.html":{"url":"opscontroller-task.html","title":"Task","keywords":"","body":"Ops-controller-manager Task Object The Task object in the Ops Controller defines the operations or tasks that need to be executed, either on specific hosts or within the Ops Controller pod. You can create and manage Task objects using opscli commands or YAML files. Create Task Using opscli Command To create a Task object directly using the create sub-command: /usr/local/bin/opscli create task --name t1 --typeref host --nameref dev1 --filepath ./task/get-osstaus.yaml name: The name of the task (e.g., t1). typeref: Specifies the type of resource the task will execute on (e.g., host). nameref: Specifies the name of the host (e.g., dev1). filepath: The path to the YAML file containing the task definition. Create Task Using YAML File Alternatively, you can define the Task object in a YAML file and apply it using kubectl. Below is an example YAML file for a scheduled task that checks the HTTP status of a URL every minute and sends a notification if the status code is not 200. apiVersion: crd.chenshaowen.com/v1 kind: Task metadata: name: alert-http-status-dockermirror namespace: ops-system spec: desc: alert crontab: \"*/1 * * * *\" variables: url: default: http://1.1.1.1:5000/ expect: default: \"200\" message: default: ${url} http status is not ${expect} steps: - name: get-status content: curl -I -m 10 -o /dev/null -s -w %{http_code} ${url} - name: notification when: ${steps.get-status.output} != ${expect} content: | curl -X POST 'https://365.kdocs.cn/woa/api/v1/webhook/send?key=xxx' -H 'content-type: application/json' -d '{ \"msgtype\": \"text\", \"text\": { \"content\": \"${message}\" } }' In this YAML: desc: A description of the task (e.g., alert). crontab: The cron schedule (e.g., */1 * * * * means every minute). variables: Defines task-specific variables, like url, expect, and message. steps: Defines the individual operations or commands that should be executed: get status: Executes a curl command to get the HTTP status code. The output is automatically available as ${output}, ${result}, or ${steps.get-status.output}. notifaction: Sends a notification if the HTTP status code does not match the expected value. Export Results from Task Steps Task steps can export results that can be referenced by other tasks in a Pipeline using path references like tasks.{taskName}.results.{resultKey}. Default Output Variable: Each step's output is automatically available in subsequent steps. You can reference them directly or use path syntax: ${output} or ${result} - references the previous step's output (direct reference, recommended) ${steps.{stepName}.output} - references a specific step's output by name (path reference) Example: steps: - name: get-status content: curl -I -m 10 -o /dev/null -s -w %{http_code} ${url} - name: notification when: ${output} != ${expect} # Direct reference (recommended) content: echo \"Status is ${output}\" Or using path syntax: steps: - name: get-status content: curl -I -m 10 -o /dev/null -s -w %{http_code} ${url} - name: notification when: ${steps.get-status.output} != ${expect} # Path reference content: echo \"Status is ${steps.get-status.output}\" Export Results Using OPS_RESULT Marker: To export specific results for use in Pipeline tasks, use the OPS_RESULT: marker in your step output: steps: - name: build-step content: | docker build -t myapp:v1.0.0 . echo \"OPS_RESULT:image=registry.example.com/myapp:v1.0.0\" echo \"OPS_RESULT:tag=v1.0.0\" Supported formats: OPS_RESULT:key=value OPS_RESULT:key:value OPS_RESULT:{\"key\":\"value\"} (JSON format for multiple results) Export Results Using key:value Format (Backward Compatible): The last step's output in key:value format will be automatically exported: steps: - name: build-step content: | docker build -t myapp:v1.0.0 . echo \"image:registry.example.com/myapp:v1.0.0\" You can apply this file using: kubectl apply -f task.yaml View Task Object Status To view the status of a specific Task object, use the following command: kubectl get task t1 -n ops-system To list all tasks in the ops-system namespace: kubectl get task -n ops-system Example output: NAME CRONTAB TYPEREF NAMEREF NODENAME ALL STARTTIME RUNSTATUS alert-http-status-dockermirror */1 * * * * host dev1 node1 true 2024-11-09 successed In the output: NAME: The name of the task. CRONTAB: The cron schedule for the task. TYPEREF: The type of the resource the task will run on (e.g., host). NAMEREF: The reference name of the target resource (e.g., dev1). NODENAME: The name of the node where the task is running. ALL: Indicates whether the task runs on all nodes. STARTTIME: The time the task was started. RUNSTATUS: The current status of the task (e.g., successed). "},"opscontroller-pipeline.html":{"url":"opscontroller-pipeline.html","title":"Pipeline","keywords":"","body":"Ops-controller-manager Pipeline Object The Pipeline object defines a sequence of tasks executed in a specific order. Tasks in a pipeline can reference results from previous tasks using path references. Create Pipeline Using YAML File apiVersion: crd.chenshaowen.com/v1 kind: Pipeline metadata: name: app-deploy namespace: ops-system spec: desc: \"Deploy application to production\" variables: namespace: value: \"production\" tasks: - name: prepare-config taskRef: prepare-config-task results: config: generate-step version: generate-step - name: deploy-app taskRef: deploy-app-task variables: config: ${config} version: ${version} - name: verify-deploy taskRef: verify-deploy-task variables: version: ${version} In this YAML: desc: A description of the pipeline. variables: Pipeline-level variables. tasks: List of tasks to execute in order: name: Task name in the pipeline (used for path references). taskRef: Reference to the Task object. results: Map of result keys to step names, defining which step outputs to export. variables: Task-specific variables (automatically filled by controller). If a task variable has a default value and the pipeline has the same variable, it will be automatically filled here. Results from previous tasks are also automatically available as variables. Automatic Variable Filling: When a Pipeline is created or updated, the controller automatically fills variables for each task: If a task variable has a default value and the pipeline has the same variable, the pipeline value is filled into TaskRef.variables This makes all variable values visible in the Pipeline spec Variable References Results from previous tasks are automatically added to Pipeline variables, so you can reference them directly: variables: config: ${config} # Direct reference (recommended) version: ${version} # Direct reference (recommended) You can also use path syntax for explicit references: variables: config: ${tasks.prepare-config.results.config} # Path reference version: ${tasks.prepare-config.results.version} # Path reference View Pipeline Object kubectl get pipeline -n ops-system "},"opscontroller-metrics.html":{"url":"opscontroller-metrics.html","title":"Metrics","keywords":"","body":"Ops Metrics Ops exposes Prometheus metrics for monitoring controller and server components. Metrics Endpoints Controller: :9090/metrics Server: :9090/metrics Controller Metrics Resource Info Metrics These metrics expose resource information during each reconcile. Metric Labels Description ops_controller_task_info namespace, name, desc, host, runtime_image Task resource info (static fields only) ops_controller_task_status namespace, name Task resource status (dynamic fields) ops_controller_pipeline_info namespace, name, desc Pipeline resource info (static fields only) ops_controller_pipeline_status namespace, name Pipeline resource status (dynamic fields) ops_controller_host_info namespace, name, address Host resource info (static fields only) ops_controller_host_status namespace, name, hostname, distribution, arch, status Host resource status (dynamic fields) ops_controller_cluster_info namespace, name, server Cluster resource info (static fields only) ops_controller_cluster_status namespace, name, version, status, node, pod_count, running_pod, cert_not_after_days Cluster resource status (dynamic fields) ops_controller_eventhooks_info namespace, name, type, subject, url EventHooks resource info (static fields only) ops_controller_eventhooks_status namespace, name, keyword, event_id EventHooks resource status (dynamic fields, including trigger information) TaskRun Metrics Metric Labels Description ops_controller_taskrun_info namespace, name, taskref, crontab TaskRun resource info (static fields only) ops_controller_taskrun_status namespace, name, status TaskRun resource status (dynamic fields) ops_controller_taskrun_start_time namespace, name, taskref TaskRun start time (unix timestamp) ops_controller_taskrun_end_time namespace, name, taskref TaskRun end time (unix timestamp) ops_controller_taskrun_duration_seconds namespace, name, taskref, status TaskRun duration in seconds PipelineRun Metrics Metric Labels Description ops_controller_pipelinerun_info namespace, name, pipelineref, crontab PipelineRun resource info (static fields only) ops_controller_pipelinerun_status namespace, name, status PipelineRun resource status (dynamic fields) ops_controller_pipelinerun_start_time namespace, name, pipelineref PipelineRun start time (unix timestamp) ops_controller_pipelinerun_end_time namespace, name, pipelineref PipelineRun end time (unix timestamp) ops_controller_pipelinerun_duration_seconds namespace, name, pipelineref, status PipelineRun duration in seconds Run Count Metrics Run counts can be calculated from _info and _status metrics: TaskRun total by taskref and status: count by (taskref, status) (ops_controller_taskrun_info{namespace=\"$namespace\"} == 1) * on(namespace, name) group_left(status) ops_controller_taskrun_status{namespace=\"$namespace\"} PipelineRun total by pipelineref and status: count by (pipelineref, status) (ops_controller_pipelinerun_info{namespace=\"$namespace\"} == 1) * on(namespace, name) group_left(status) ops_controller_pipelinerun_status{namespace=\"$namespace\"} EventHooks Metrics EventHooks trigger information is recorded in ops_controller_eventhooks_status metric with keyword and event_id labels. Reconcile Metrics Metric Labels Description ops_controller_reconcile_total controller, namespace, result Total number of reconcile operations ops_controller_reconcile_errors_total controller, namespace, error_type Total number of reconcile errors Controller Resource Metrics Metric Labels Description ops_controller_resource_goroutines pod Controller number of goroutines ops_controller_resource_cpu_usage_seconds_total pod Controller CPU usage in seconds (cumulative, read from cgroup) ops_controller_resource_memory_usage_bytes pod Controller memory usage in bytes (read from cgroup) ops_controller_uptime_seconds pod Controller uptime in seconds ops_controller_info pod, version, build_date Controller information Server Metrics Resource Metrics Metric Labels Description ops_server_resource_goroutines pod Server number of goroutines ops_server_resource_cpu_usage_seconds_total pod Server CPU usage in seconds (cumulative, read from cgroup) ops_server_resource_memory_usage_bytes pod Server memory usage in bytes (read from cgroup) Throughput Metrics Metric Labels Description ops_server_throughput_http_requests_total method, path, status_code Total number of HTTP requests ops_server_throughput_api_requests_total endpoint, namespace, status Total number of API requests ops_server_throughput_api_errors_total endpoint, namespace, error_type Total number of API errors Server Info Metrics Metric Labels Description ops_server_info pod, version, build_date Server information ops_server_uptime_seconds pod Server uptime in seconds Example Queries Get all running TaskRuns ops_controller_taskrun_info{status=\"Running\"} Get Task run count by status count by (taskref, status) ( ops_controller_taskrun_info{namespace=\"$namespace\"} == 1 * on(namespace, name) group_left(status) ops_controller_taskrun_status{namespace=\"$namespace\"} ) Get Host list with status ops_controller_host_info Get EventHooks triggers by keyword count by (name, keyword) (ops_controller_eventhooks_status{namespace=\"$namespace\",keyword!=\"\"}) Get TaskRun duration ops_controller_taskrun_duration_seconds Get Controller CPU usage rate rate(ops_controller_resource_cpu_usage_seconds_total{pod=\"xxx\"}[5m]) Get Controller memory usage ops_controller_resource_memory_usage_bytes{pod=\"xxx\"} Get Server CPU usage rate rate(ops_server_resource_cpu_usage_seconds_total{pod=\"xxx\"}[5m]) Get Server memory usage ops_server_resource_memory_usage_bytes{pod=\"xxx\"} "},"opscontroller-events.html":{"url":"opscontroller-events.html","title":"Events","keywords":"","body":"Ops Event System Overview The Ops project publishes various events through NATS JetStream for monitoring, alerting, and integration. All events follow the CloudEvents standard and are routed through NATS subjects. Event Subject Format Standard Format Most event subjects follow this format: ops.clusters.{cluster}.namespaces.{namespace}.{resourceType}.{resourceName}.{observation} Where: {cluster}: Cluster name (from environment variable EVENT_CLUSTER) {namespace}: Kubernetes namespace {resourceType}: Resource type (e.g., hosts, clusters, taskruns, etc.) {resourceName}: Resource name {observation}: Observation type, possible values and meanings: status: Resource status information (e.g., running status, health status, etc.) metrics: Metrics data (e.g., CPU, memory, disk usage, etc.) logs: Log information events: Event information (e.g., Kubernetes events) traces: Tracing information (e.g., distributed tracing data) alerts: Alert information findings: Proactively reported information and status Node Event Special Format Node events use a special format without the namespaces part: ops.clusters.{cluster}.nodes.{nodeName}.{observation} Example: ops.clusters.mycluster.nodes.mynode.events Note: Nodes are cluster-level resources and do not belong to any namespace Node event subjects omit the namespaces part Other format rules are the same as the standard format Event Types 1. Controller Setup Events Subject Format: ops.clusters.{cluster}.namespaces.{namespace}.controllers.{resourceType}.status Trigger: Published when each Controller starts, indicating the Controller is ready Published From: controllers/host_controller.go: Host Controller startup controllers/cluster_controller.go: Cluster Controller startup controllers/task_controller.go: Task Controller startup controllers/taskrun_controller.go: TaskRun Controller startup controllers/pipeline_controller.go: Pipeline Controller startup controllers/pipelinerun_controller.go: PipelineRun Controller startup controllers/event_controller.go: Event Controller startup controllers/eventhooks_controller.go: EventHook Controller startup Event Data Structure: { \"cluster\": \"string\", \"kind\": \"string\" // Resource type, e.g., \"Hosts\", \"Clusters\", \"TaskRuns\", etc. } 2. Host Status Events Subject Format: ops.clusters.{cluster}.namespaces.{namespace}.hosts.{hostName}.status Trigger: When Host resource status is updated (heartbeat status, disk usage, etc.) Published From: controllers/host_controller.go: When Host status changes Event Data Structure: { \"cluster\": \"string\", \"address\": \"string\", \"port\": 0, \"username\": \"string\", \"status\": { \"hostname\": \"string\", \"diskUsagePercent\": \"string\", \"heartStatus\": \"string\", \"heartTime\": \"string\", // ... other HostStatus fields } } 3. Cluster Status Events Subject Format: ops.clusters.{cluster}.namespaces.{namespace}.clusters.{clusterName}.status Trigger: When Cluster resource status is updated (Kubernetes version, certificate expiration, heartbeat status, etc.) Published From: controllers/cluster_controller.go: When Cluster status changes Event Data Structure: { \"cluster\": \"string\", \"server\": \"string\", \"status\": { \"version\": \"string\", \"certNotAfterDays\": 0, \"heartStatus\": \"string\", \"heartTime\": \"string\", // ... other ClusterStatus fields } } 4. TaskRun Status Events Subject Format: ops.clusters.{cluster}.namespaces.{namespace}.taskruns.{taskRunName}.status Trigger: When TaskRun execution completes or status changes Published From: controllers/taskrun_controller.go: When TaskRun execution completes (in run function) When TaskRun status changes (in Reconcile) Event Data Structure: { \"cluster\": \"string\", \"taskRef\": \"string\", \"desc\": \"string\", \"variables\": { \"key\": \"value\" }, \"runStatus\": \"string\", \"startTime\": \"string\", \"taskRunNodeStatus\": [ { \"nodeName\": \"string\", \"runStatus\": \"string\", \"taskRunStep\": [ { \"stepName\": \"string\", \"stepContent\": \"string\", \"stepOutput\": \"string\", \"stepStatus\": \"string\" } ] } ] } 5. PipelineRun Status Events Subject Format: ops.clusters.{cluster}.namespaces.{namespace}.pipelineruns.{pipelineRunName}.status Trigger: When PipelineRun execution completes or status changes When cross-cluster PipelineRun status synchronization completes Published From: controllers/pipelinerun_controller.go: When PipelineRun execution completes (in run function) When PipelineRun status changes (in Reconcile) When cross-cluster PipelineRun status synchronization completes (in goroutine) Event Data Structure: { \"cluster\": \"string\", \"pipelineRef\": \"string\", \"desc\": \"string\", \"variables\": { \"key\": \"value\" }, \"runStatus\": \"string\", \"startTime\": \"string\", \"pipelineRunStatus\": [ { \"taskName\": \"string\", \"taskRef\": \"string\", \"taskRunStatus\": { \"runStatus\": \"string\", \"taskRunNodeStatus\": [] } } ] } 6. Kubernetes Events Subject Format: For namespaced resources: ops.clusters.{cluster}.namespaces.{namespace}.{resourceKind}s.{resourceName}.events For Node resources (special format, no namespaces): ops.clusters.{cluster}.nodes.{nodeName}.events Trigger: When Kubernetes native events (events.k8s.io/v1) are created, updated, or deleted Only events from the last 2 minutes are published Published From: controllers/event_controller.go: Watches Kubernetes Event resources Event Data Structure: { \"cluster\": \"string\", \"type\": \"string\", // Normal, Warning, etc. \"reason\": \"string\", // Event reason \"eventTime\": \"string\", // ISO 8601 format \"from\": \"string\", // Event source (Manager) \"message\": \"string\" // Event message } Example Subjects: Namespaced resources: ops.clusters.mycluster.namespaces.ops-system.pods.my-pod.events ops.clusters.mycluster.namespaces.ops-system.deployments.my-deployment.events Node resources (special format): ops.clusters.mycluster.nodes.mynode.events 7. Notification Events Subject Format: ops.notifications.providers.{provider}.channels.{channel}.severities.{severity} Where: {provider}: Notification provider or system name (e.g., ksyun, ai, etc.) {channel}: Notification channel type (e.g., webhook, email, sms, etc.) {severity}: Severity level (e.g., info, warning, error, critical, etc.) Trigger: Published when the notification system sends notifications Published From: Published via API endpoint /api/v1/namespaces/{namespace}/events/{event} If the event path starts with ops., it will be used directly as the subject without format transformation Publishing Method: curl -X POST http://localhost:80/api/v1/namespaces/ops-system/events/ops.notifications.providers.ksyun.channels.webhook.severities.info \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer YOUR_TOKEN\" \\ -d '{ \"provider\": \"ksyun\", \"channel\": \"webhook\", \"severity\": \"info\", \"message\": \"Notification message\", \"title\": \"Notification title\" }' Event Data Structure: { \"provider\": \"string\", \"channel\": \"string\", \"severity\": \"string\", \"message\": \"string\", \"title\": \"string\", \"timestamp\": \"string\", // ... other notification-related fields } Example Subjects: ops.notifications.providers.ksyun.channels.webhook.severities.info ops.notifications.providers.ksyun.channels.webhook.severities.warning ops.notifications.providers.ksyun.channels.webhook.severities.error ops.notifications.providers.ksyun.channels.email.severities.critical ops.notifications.providers.AI.channels.webhook.severities.info Note: Notification events use an independent subject format without clusters and namespaces parts Used for notification system routing and distribution Supports combinations of multiple providers, channels, and severity levels Important: If the event path starts with ops., the API will use the path directly as the NATS subject without any format transformation 8. Custom Events (via API) Subject Format: The API uses different processing methods based on the event path: Event path starting with ops. (direct delivery): ops.{any path} Uses the path directly as the NATS subject without any format transformation Suitable for notification events and other independent format events Event path starting with nodes. (node events): ops.clusters.{cluster}.nodes.{nodeName}.{observation} Converts to node event format without the namespaces part Standard format (other paths): ops.clusters.{cluster}.namespaces.{namespace}.{eventName} Automatically adds cluster and namespace prefixes Trigger: Manually published via API endpoint /api/v1/namespaces/{namespace}/events/{event} Published From: pkg/server/api.go: CreateEvent function Event Data Structure: Custom JSON object defined by the caller Usage Examples: Standard format event: curl -X POST http://localhost:80/api/v1/namespaces/ops-system/events/my-custom-event \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer YOUR_TOKEN\" \\ -d '{ \"message\": \"Custom event data\", \"level\": \"info\" }' Direct delivery format (starting with ops.): curl -X POST http://localhost:80/api/v1/namespaces/ops-system/events/ops.notifications.providers.ksyun.channels.webhook.severities.info \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer YOUR_TOKEN\" \\ -d '{ \"provider\": \"ksyun\", \"channel\": \"webhook\", \"severity\": \"info\", \"message\": \"Notification message\" }' Node event format: curl -X POST http://localhost:80/api/v1/namespaces/ops-system/events/nodes.mynode.findings \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer YOUR_TOKEN\" \\ -d '{ \"message\": \"Node finding\", \"status\": \"normal\" }' Event Subscription Examples Subscribe to All Events nats --user=app --password=${apppassword} sub \"ops.>\" Subscribe to Events in a Specific Namespace nats --user=app --password=${apppassword} sub \"ops.clusters.*.namespaces.ops-system.>\" Subscribe to Host Status Events nats --user=app --password=${apppassword} sub \"ops.clusters.*.namespaces.*.hosts.*.status\" Subscribe to TaskRun Status Events nats --user=app --password=${apppassword} sub \"ops.clusters.*.namespaces.*.taskruns.*.status\" Subscribe to PipelineRun Status Events nats --user=app --password=${apppassword} sub \"ops.clusters.*.namespaces.*.pipelineruns.*.status\" Subscribe to Kubernetes Events # Subscribe to Kubernetes events for all namespaced resources nats --user=app --password=${apppassword} sub \"ops.clusters.*.namespaces.*.*.events\" # Subscribe to node events (special format) nats --user=app --password=${apppassword} sub \"ops.clusters.*.nodes.*.events\" Subscribe to Notification Events # Subscribe to all notification events nats --user=app --password=${apppassword} sub \"ops.notifications.>\" # Subscribe to notifications from a specific provider nats --user=app --password=${apppassword} sub \"ops.notifications.providers.ksyun.>\" # Subscribe to notifications from a specific channel nats --user=app --password=${apppassword} sub \"ops.notifications.providers.*.channels.webhook.>\" # Subscribe to notifications with a specific severity level nats --user=app --password=${apppassword} sub \"ops.notifications.providers.*.channels.*.severities.error\" Event Format All events follow the CloudEvents standard and include these standard fields: id: Unique event identifier (UUID) source: Event source (fixed as https://github.com/shaowenchen/ops) type: Event type (e.g., Host, Cluster, PipelineRun, etc.) specversion: CloudEvents specification version (1.0) time: Event timestamp (ISO 8601 format) data: Event data (JSON format) subject: NATS subject (automatically set) Environment Variables The event system depends on the following environment variables: EVENT_CLUSTER: Cluster name, used to build event subjects EVENT_ENDPOINT: NATS server address (e.g., nats://nats:4222) Notes Event Deduplication: NATS JetStream supports message deduplication via dupe-window configuration (default 2 minutes) Event Retention: Event retention time is determined by Stream configuration (default 24 hours) Event Ordering: Events on the same subject are published in chronological order Async Publishing: Most events are published asynchronously via goroutines, not blocking the main flow Event Filtering: Kubernetes events only publish events from the last 2 minutes to avoid historical event interference Related Documentation NATS Configuration EventHooks Usage Guide "},"opscontroller-eventhooks.html":{"url":"opscontroller-eventhooks.html","title":"EventHooks","keywords":"","body":"EventHooks Usage Guide Overview EventHooks is an event notification mechanism provided by Ops that allows you to forward events to different notification channels based on keyword matching rules. It supports multiple notification types including Webhook, Collaborative Document, Event Forwarding, and Elasticsearch. Basic Concepts EventHooks Resource EventHooks is a Kubernetes CRD resource used to define event notification rules: apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: my-eventhook namespace: ops-system spec: type: webhook # Notification type subject: ops.clusters.*.namespaces.*.pods.*.events # Event subject to subscribe url: https://example.com/webhook # Notification target URL keywords: # Keyword matching rules include: - \"Error\" matchType: CONTAINS Field Description type: Notification type, supported values: webhook: HTTP Webhook notification xiezuo: Collaborative document notification event: Event forwarding (forward to NATS subject) elasticsearch: Elasticsearch indexing subject: Event subject to subscribe, supports wildcards (e.g., ops.clusters.*.namespaces.*.pods.*.events) url: Notification target address, different formats required for different notification types keywords: Keyword matching configuration options: Additional configuration options, different notification types support different options Keyword Matching Match Mode (matchMode) ANY (default): Matches if any keyword in the list is found ALL: Matches only if all keywords in the list are found Match Type (matchType) CONTAINS (default): String contains matching EXACT: Exact matching REGEX: Regular expression matching Examples Example 1: Contains Matching keywords: include: - \"Error\" - \"Failed\" matchMode: ANY matchType: CONTAINS Matches events containing \"Error\" or \"Failed\". Example 2: Regular Expression Matching keywords: include: - \"(?=.*(kube|etcd|calico|csi|fluid)).*(BackOff|OOMKilled|Evicted|NetworkNotReady|Unhealthy|Error|Failed|ImagePullBackOff).*\" matchType: REGEX Using regular expression to match: Must contain: any one of kube, etcd, calico, csi, or fluid And must contain: any one of BackOff, OOMKilled, Evicted, etc. Example 3: Exclude Matching keywords: include: - \"Error\" exclude: - \"healthcheck\" - \"test\" matchType: CONTAINS Matches events containing \"Error\" but not containing \"healthcheck\" or \"test\". Notification Types 1. Webhook Notification (webhook) Sends event data in JSON format to the specified HTTP Webhook URL. Configuration Example: apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: webhook-notification namespace: ops-system spec: type: webhook subject: ops.clusters.*.namespaces.*.pods.*.events url: https://example.com/api/webhook keywords: include: - \"Error\" matchType: CONTAINS Data Format Sent: Readable string format of the event (contains all event field information). 2. Collaborative Document Notification (xiezuo) Sends to collaborative document system webhook. Configuration Example: apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: xiezuo-notification namespace: ops-system spec: type: xiezuo subject: ops.clusters.*.namespaces.*.pods.*.events url: https://365.kdocs.cn/woa/api/v1/webhook/send?key=xxxx keywords: include: - \"Error\" matchType: CONTAINS Data Format: If data is already in XiezuoBody format, send directly; otherwise convert to text format. 3. Event Forwarding (event) Forwards events to another NATS subject. Supports wildcard replacement. Configuration Example: apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: event-forward namespace: ops-system spec: type: event subject: ops.clusters.*.namespaces.*.pods.*.events # Use wildcard * in URL, automatically replaced with corresponding values from original event subject url: ops.clusters.*.namespaces.*.pods.*.alerts keywords: include: - \"(?=.*(kube|etcd|calico)).*(BackOff|OOMKilled|Evicted|Error|Failed).*\" matchType: REGEX Wildcard Replacement Example: Original event subject: ops.clusters.cluster1.namespaces.ns1.pods.pod1.events URL template: ops.clusters.*.namespaces.*.pods.*.alerts Replaced: ops.clusters.cluster1.namespaces.ns1.pods.pod1.alerts Node Event Support: Also supports node event format: Original event: ops.clusters.cluster1.nodes.node1.events URL template: ops.clusters.*.nodes.*.alerts Replaced: ops.clusters.cluster1.nodes.node1.alerts Notes: Forwarded events will have new ID and timestamp Need to set environment variable EVENT_ENDPOINT to specify NATS server address 4. Elasticsearch Notification (elasticsearch) Indexes event data to Elasticsearch. Configuration Example: apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: elasticsearch-notification namespace: ops-system spec: type: elasticsearch subject: ops.clusters.*.namespaces.*.pods.*.events url: http://elasticsearch:9200/ops-events/_doc options: username: elastic # Elasticsearch username (optional) password: changeme # Elasticsearch password (optional) index: ops-events # Override index name from URL (optional) id: \"\" # Document ID (optional, auto-generated if not specified) keywords: include: - \"Error\" matchType: CONTAINS URL Format: http://host:port/index/_doc - Auto-generate document ID http://host:port/index/_doc/doc-id - Specify document ID http://host:port - Need to specify index in options Date-based Index Names: You can use date placeholders in the index name to create time-based indices. Supported placeholders: {date} or {YYYY.MM.DD} -> 2024.01.19 {YYYY-MM-DD} -> 2024-01-19 {YYYYMMDD} -> 20240119 {YYYY.MM} -> 2024.01 (monthly index) {YYYY-MM} -> 2024-01 (monthly index) {YYYY} -> 2024 (yearly index) Example with Date-based Index: spec: type: elasticsearch subject: ops.clusters.*.namespaces.*.pods.*.events url: http://elasticsearch:9200 options: index: ops-events-{YYYY.MM.DD} # Creates index like ops-events-2024.01.19 username: elastic password: changeme This will create daily indices like: ops-events-2024.01.19 ops-events-2024.01.20 etc. Indexed Document Structure: The indexed document contains only the original event data from event.Data(). No additional metadata or extension fields are added. Example: If the original event data is: { \"cluster\": \"cluster1\", \"namespace\": \"ns1\", \"pod\": \"pod1\", \"type\": \"Warning\", \"reason\": \"BackOff\", \"message\": \"Event message\" } Then the indexed document will be exactly the same: { \"cluster\": \"cluster1\", \"namespace\": \"ns1\", \"pod\": \"pod1\", \"type\": \"Warning\", \"reason\": \"BackOff\", \"message\": \"Event message\" } Note: Only the original event data is indexed. No metadata fields (like @timestamp, event_id, event_type, etc.) or extension fields (like ext_*) are added to the document. Supported Options: username: Elasticsearch username (for Basic auth) password: Elasticsearch password (for Basic auth) index: Override index name from URL (supports date placeholders like {YYYY.MM.DD}) id: Document ID (if not specified in URL) Complete Examples Example 1: Send Pod Error Events to Webhook apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: pod-errors-to-webhook namespace: ops-system spec: type: webhook subject: ops.clusters.*.namespaces.*.pods.*.events url: https://alerting.example.com/webhook keywords: include: - \"(?=.*(kube|etcd|calico|csi|fluid)).*(BackOff|OOMKilled|Evicted|NetworkNotReady|Unhealthy|Error|Failed|ImagePullBackOff).*\" matchType: REGEX Example 2: Forward Events to Alert Subject apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: convert-events-to-alerts namespace: ops-system spec: type: event subject: ops.clusters.*.namespaces.*.pods.*.events url: ops.clusters.*.namespaces.*.pods.*.alerts keywords: include: - \"(?=.*(kube|etcd|calico|csi|fluid)).*(BackOff|OOMKilled|Evicted|NetworkNotReady|Unhealthy|Error|Failed|ImagePullBackOff).*\" matchType: REGEX Example 3: Index Events to Elasticsearch apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: events-to-elasticsearch namespace: ops-system spec: type: elasticsearch subject: ops.clusters.*.namespaces.*.pods.*.events url: http://elasticsearch:9200/ops-events/_doc options: username: elastic password: changeme keywords: include: - \"Error\" matchType: CONTAINS Example 3b: Index Events to Elasticsearch with Date-based Indices apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: events-to-elasticsearch-daily namespace: ops-system spec: type: elasticsearch subject: ops.clusters.*.namespaces.*.pods.*.events url: http://elasticsearch:9200 options: index: ops-events-{YYYY.MM.DD} # Creates daily indices like ops-events-2024.01.19 username: elastic password: changeme keywords: include: - \"Error\" matchType: CONTAINS Example 4: Node Event Processing apiVersion: crd.chenshaowen.com/v1 kind: EventHooks metadata: name: node-events-to-alerts namespace: ops-system spec: type: event subject: ops.clusters.*.nodes.*.events url: ops.clusters.*.nodes.*.alerts keywords: include: - \"(?=.*(kube|etcd|calico)).*(BackOff|OOMKilled|Error|Failed).*\" matchType: REGEX Best Practices Use Regular Expressions for Complex Matching: When matching multiple conditions, use regular expressions for more precise control. Use Exclude Rules Appropriately: Use exclude to filter out unwanted events and reduce false positives. Use Wildcards for Event Forwarding: In event forwarding scenarios, use wildcards to maintain event subject structure for subsequent processing. Elasticsearch Index Naming: Use meaningful index names, consider organizing by date or cluster name. Monitor EventHooks Status: Monitor EventHooks trigger status through Prometheus metrics to detect configuration issues promptly. Troubleshooting View EventHooks Status kubectl get eventhooks -n ops-system kubectl describe eventhooks -n ops-system View Controller Logs kubectl logs -n ops-system deployment/ops-controller-manager | grep eventhook Check Metrics # View EventHooks trigger count curl http://localhost:8080/metrics | grep ops_controller_eventhooks_status Common Issues Events Not Triggering: Check if subject correctly matches event subjects Check if keywords configuration is correct View Controller logs for errors Webhook Send Failure: Check if URL is accessible Check network connection View error messages in Controller logs Elasticsearch Index Failure: Check Elasticsearch connection Verify authentication credentials Check index permissions Related Documentation Event System Documentation Metrics Documentation "},"nats.html":{"url":"nats.html","title":"Nats","keywords":"","body":"NATS Purpose Ops uses the NATS component to export relevant events, primarily of two types: The status of CRDs, including the status of hosts, clusters, TaskRun, and PipelineRun. Status information reported by scheduled inspections from alerts. Below is a guide for installing and configuring the NATS component. This setup follows a model with one primary cluster and multiple edge clusters. The edge clusters forward events to the primary cluster for unified processing. Adding the Helm Repo Add the repository: helm repo add nats https://nats-io.github.io/k8s/helm/charts/ helm repo update View configurable fields: helm show values nats/nats Deploying the Primary Cluster Set basic NATS credentials: export adminpassword=mypassword export apppassword=mypassword Generate nats-values.yaml: cat nats-values.yaml config: jetstream: enabled: true fileStore: enabled: false dir: /data memoryStore: enabled: true maxSize: 1Gi pvc: enabled: false storageClassName: my-sc cluster: enabled: true leafnodes: enabled: true merge: accounts: SYS: users: - user: admin password: ${adminpassword} APP: users: - user: app password: ${apppassword} jetstream: true system_account: SYS container: image: repository: nats tag: 2.10.20-alpine natsBox: container: image: repository: nats-box tag: 0.14.5 reloader: enabled: true image: repository: natsio/nats-server-config-reloader tag: 0.15.1 EOF Data is persisted in memory. To store it on disk, you need to configure fileStore. Install NATS: helm -n ops-system install nats nats/nats --version 1.2.4 -f nats-values.yaml Uninstall NATS: helm -n ops-system uninstall nats Expose the NATS service ports: kubectl patch svc nats -p '{\"spec\":{\"type\":\"NodePort\",\"ports\":[{\"port\":4222,\"nodePort\":32223,\"targetPort\":\"nats\"},{\"port\":7422,\"nodePort\":32222,\"targetPort\":\"leafnodes\"}]}}' -n ops-system Check the workload: kubectl -n ops-system get pod,svc | grep nats pod/nats-0 2/2 Running 0 15h pod/nats-1 2/2 Running 0 15h pod/nats-2 2/2 Running 0 15h pod/nats-box-6bb86df889-xcr6x 1/1 Running 0 15h service/nats NodePort 10.100.109.24 4222:32223/TCP,7422:32222/TCP 15h service/nats-headless ClusterIP None 4222/TCP,7422/TCP,6222/TCP,8222/TCP 15h Deploying Edge Clusters Add the repository: helm repo add nats https://nats-io.github.io/k8s/helm/charts/ helm repo update Set the primary cluster's NATS information: export natsendpoint=x.x.x.x:32222 Set the NATS server name: export natsservername=need-to-be-unique Generate nats-values.yaml: Note that the server_name must be unique for each cluster; otherwise, duplicate connection issues will arise. cat nats-values.yaml config: leafnodes: enabled: true merge: remotes: - urls: - nats://admin:${adminpassword}@${natsendpoint} account: SYS - urls: - nats://app:${apppassword}@${natsendpoint} account: APP merge: server_name: ${natsservername} accounts: SYS: users: - user: admin password: ${adminpassword} APP: users: - user: app password: ${apppassword} jetstream: true system_account: SYS container: image: repository: nats tag: 2.10.20-alpine natsBox: container: image: repository: nats-box tag: 0.14.5 reloader: enabled: true image: repository: natsio/nats-server-config-reloader tag: 0.15.1 EOF Install NATS: helm install nats nats/nats --version 1.2.4 -f nats-values.yaml -n ops-system Common NATS Commands Test NATS: kubectl -n ops-system exec -it deployment/nats-box -- sh Subscribe to messages: nats --user=app --password=${apppassword} sub \"ops.>\" Publish messages: nats --user=app --password=${apppassword} pub ops.test \"mymessage mycontent\" Delete stream: nats --user=app --password=${apppassword} stream rm ops Create stream: nats --user=app --password=${apppassword} stream add ops --subjects \"ops.>\" --ack --max-msgs=-1 --max-bytes=-1 --max-age=24h --storage file --retention limits --max-msg-size=-1 --discard=old --replicas 1 --dupe-window=2m For production environments, it is recommended to use file storage and set replicas to 3. View stream events: nats --user=app --password=${apppassword} stream view ops View stream configuration: nats --user=app --password=${apppassword} stream info ops View cluster information: nats --user=admin --password=${adminpassword} server report jetstream This command displays information about the primary cluster, edge clusters, and their connections. View the subjects of a stream: nats --user=app --password=${apppassword} stream subjects ops Perform a benchmark: nats --user=app --password=${apppassword} bench benchsubject --pub 1 --sub 10 References JetStream Configuration LeafNode Configuration Gateway Configuration "}}